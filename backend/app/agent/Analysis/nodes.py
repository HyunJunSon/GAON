# app/agent/Analysis/nodes.py 
# =========================================
# í˜•íƒœì†Œ ê¸°ë°˜ MATTR + í™”ìžë³„ other í†µê³„ ë¶„ë¦¬
# =========================================

from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple
from app.core.config import settings
from langchain_openai import ChatOpenAI
import pandas as pd
from sqlalchemy.orm import Session
from collections import Counter
import re
import json

# ðŸ§© í˜•íƒœì†Œ ë¶„ì„ê¸° ì¶”ê°€
from konlpy.tag import Okt
okt = Okt()

# =========================================
# CRUD import
# =========================================
from app.agent.crud import (
    get_user_by_id,
    save_analysis_result,
)


# =========================================
# ðŸ”§ CONTENT WORD EXTRACTOR
# =========================================
def extract_content_words_korean(text: str) -> List[str]:
    """í•œêµ­ì–´ ë‚´ìš©ì–´(ëª…ì‚¬Â·ë™ì‚¬Â·í˜•ìš©ì‚¬Â·ë¶€ì‚¬)ë§Œ ì¶”ì¶œ"""
    morphs = okt.pos(text, stem=True)
    content_pos = ["Noun", "Verb", "Adjective", "Adverb"]
    return [word for word, pos in morphs if pos in content_pos]


def calculate_mattr_korean(words: List[str], window_size: int = 25) -> float:
    """í•œêµ­ì–´ ë‚´ìš©ì–´ ê¸°ë°˜ MATTR ê³„ì‚°"""
    if len(words) < window_size:
        return len(set(words)) / len(words) if words else 0.0

    ttr_vals = []
    for i in range(len(words) - window_size + 1):
        window = words[i:i + window_size]
        ttr_vals.append(len(set(window)) / window_size)

    return sum(ttr_vals) / len(ttr_vals)


# =========================================
# UserFetcher
# =========================================
@dataclass
class UserFetcher:
    def fetch(self, db: Session, conv_state) -> Dict[str, Any]:
        id = conv_state.id
        if not id:
            raise ValueError("âŒ UserFetcher: idê°€ ì—†ìŠµë‹ˆë‹¤.")

        user = get_user_by_id(db, id)
        if not user:
            raise ValueError(f"âŒ UserFetcher: id={id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print(f"âœ… [UserFetcher] ì‚¬ìš©ìž ì¡°íšŒ: {user.get('user_name')}")
        return user


# =========================================
# Analyzer 
# =========================================
@dataclass
class Analyzer:
    verbose: bool = False

    def analyze(
        self,
        conversation_df: pd.DataFrame,
        relations: List[Dict[str, Any]],
        id: int
    ) -> Dict[str, Any]:

        llm = ChatOpenAI(model="gpt-4o-mini", api_key=settings.openai_api_key)

        # ë¶„ì„ ëŒ€ìƒ ì‚¬ìš©ìž DF
        user_df = conversation_df[conversation_df["speaker"] == int(id)]
        if user_df.empty:
            raise ValueError(f"âŒ id={id}ì˜ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # í™”ìžë³„ ë¶„ë¦¬ â†’ otherë¥¼ í•œë° ëª¨ìœ¼ì§€ ì•ŠìŒ
        other_speakers = sorted(
            list(set(conversation_df["speaker"].tolist()) - {int(id)})
        )

        others_grouped_stats = {}
        for spk in other_speakers:
            spk_df = conversation_df[conversation_df["speaker"] == spk]
            spk_text = " ".join(spk_df["text"].tolist())
            spk_words = extract_content_words_korean(spk_text)
            others_grouped_stats[str(spk)] = {
                "token_count": len(spk_words),
                "mattr": calculate_mattr_korean(spk_words),
                "unique_content_words": len(set(spk_words)),
                "top_content_words": Counter(spk_words).most_common(5)
            }

        # ðŸ”§ user í†µê³„ ê³„ì‚°
        user_text = " ".join(user_df["text"].tolist())
        user_words = extract_content_words_korean(user_text)

        user_stats = {
            "token_count": len(user_words),
            "mattr": calculate_mattr_korean(user_words),
            "unique_content_words": len(set(user_words)),
            "top_content_words": Counter(user_words).most_common(5)
        }

        # ðŸ”§ comparison
        comparison = {
            "user_mattr": user_stats["mattr"],
            "others_mattr": {
                spk: stats["mattr"] for spk, stats in others_grouped_stats.items()
            }
        }

        # ðŸ”§ ì „ì²´ statistics JSON
        statistics = {
            "user": user_stats,
            "others": others_grouped_stats,
            "comparison": comparison
        }

        # ================================
        # ìŠ¤íƒ€ì¼ ë¶„ì„ (ðŸ”¥ í†µê³„ ê²°ê³¼ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°•í™”)
        # ================================
        full_context = "\n".join([
            f"í™”ìž {row['speaker']}: {row['text']}"
            for _, row in conversation_df.iterrows()
        ])
        user_texts_joined = "\n".join(user_df["text"].tolist())

        statistics_json_str = json.dumps(statistics, ensure_ascii=False, indent=2)

        style_prompt = f"""
ë‹¤ìŒì€ ëŒ€í™” ì „ì²´ ë‚´ìš©ìž…ë‹ˆë‹¤:

{full_context}

ê·¸ë¦¬ê³  ì•„ëž˜ëŠ” 'ì‚¬ìš©ìž ID {id}'ì˜ ë°œí™”ë§Œ ëª¨ì€ ë‚´ìš©ìž…ë‹ˆë‹¤:

{user_texts_joined}

ë˜í•œ, í˜•íƒœì†Œ ê¸°ë°˜ ë‚´ìš©ì–´ ë¶„ì„ + MATTR ê¸°ë°˜ í†µê³„ ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

{statistics_json_str}

ìœ„ì˜ ëŒ€í™” ë§¥ë½, ì‚¬ìš©ìž ë°œí™”, í†µê³„ ë¶„ì„ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬
â†’ ì‚¬ìš©ìž ID {id}ì˜ **ë§íˆ¬, í‘œí˜„ ìŠµê´€, ëŒ€í™” ìŠ¤íƒ€ì¼, ì–¸ì–´ì  ì„±í–¥**ì„ êµ¬ì¡°í™”í•˜ì—¬ ë¶„ì„í•´ ì£¼ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”:
{{
  "ë§íˆ¬": "...",
  "í‘œí˜„ìŠµê´€": "...",
  "ëŒ€í™”ìŠ¤íƒ€ì¼": "...",
  "ì–¸ì–´ì íŠ¹ì§•": "...",
  "ì¢…í•©í‰ê°€": "..."
}}
"""

        try:
            resp = llm.invoke(style_prompt)
            raw = resp.content if hasattr(resp, "content") else str(resp)
            try:
                style_json = json.loads(raw)
            except:
                style_json = {"ìš”ì•½": raw[:200]}
            style_analysis = {str(id): style_json}
        except:
            style_analysis = {str(id): {"ë¶„ì„": "ì‹¤íŒ¨"}}

        # ðŸ”§ ê¸°ì¡´ ì ìˆ˜ ê³„ì‚° ìœ ì§€
        score = self._calculate_user_score(
            user_stats,
            {"token_count": sum(o["token_count"] for o in others_grouped_stats.values())},
            {}
        )

        summary = f"[ìš”ì•½] ì‚¬ìš©ìž MATTR={user_stats['mattr']:.3f}"

        return {
            "summary": summary,
            "style_analysis": style_analysis,
            "statistics": statistics,
            "score": score,
        }

    # =========================================
    # ê¸°ì¡´ ì ìˆ˜ ê³„ì‚° ìœ ì§€
    # =========================================
    def _calculate_user_score(self, user_stats, others_stats, user_analysis):
        vocab = user_stats["unique_content_words"] / max(1, user_stats["token_count"])
        score = vocab
        return round(min(1.0, max(0.0, score)), 2)


# =========================================
# ScoreEvaluator
# =========================================
@dataclass
class ScoreEvaluator:
    def evaluate(self, result: Dict[str, Any]) -> bool:
        return result.get("score", 0) >= 0.65


# =========================================
# AnalysisSaver
# =========================================
@dataclass
class AnalysisSaver:
    verbose: bool = False

    def save(self, db: Session, result: Dict[str, Any], state):
        if not result:
            return {"status": "no_result"}

        saved = save_analysis_result(
            db=db,
            id=str(state.id),
            conv_id=str(state.conv_id),
            summary=result["summary"],
            style_analysis=result["style_analysis"],
            statistics=result["statistics"],
            score=result["score"],
            confidence_score=0.0,
            conversation_count=len(state.conversation_df) if hasattr(state, "conversation_df") else 0
        )

        return {"status": "saved", "analysis_id": saved["analysis_id"]}
