# =========================================
# app/agent/Analysis/nodes.py (FINAL)
# =========================================

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Any, List
import pandas as pd
import json
from collections import Counter

from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI

# -------------------------------
# í˜•íƒœì†Œ ë¶„ì„ê¸° (Kiwi)
# -------------------------------
from kiwipiepy import Kiwi
kiwi = Kiwi()

# -------------------------------
# Dialect / Prosody Normalizer
# -------------------------------
from app.agent.Analysis.dialect_normalizer import DialectProsodyNormalizer

# -------------------------------
# CRUD
# -------------------------------
from app.agent.crud import (
    get_user_by_id,
    save_analysis_result
)

# =========================================================
# 1) í…ìŠ¤íŠ¸ Feature Utilities
# =========================================================

def extract_content_words(text: str):
    analyses = kiwi.analyze(text)
    if not analyses:
        return []

    morphs = analyses[0][0]  # í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸

    content_pos = ["NNG", "NNP", "VV", "VA", "MAG"]

    result = []

    for m in morphs:
        # mì´ tupleì¸ì§€ í™•ì¸
        if isinstance(m, tuple) and len(m) >= 2:
            form, tag = m[0], m[1]
        elif isinstance(m, dict):
            form, tag = m.get("form"), m.get("tag")
        else:
            continue

        if tag in content_pos:
            result.append(form)

    return result


def calculate_mattr(words: List[str], window: int = 25):
    if len(words) < window:
        return len(set(words)) / len(words) if words else 0
    scores = []
    for i in range(len(words) - window + 1):
        win = words[i:i+window]
        scores.append(len(set(win)) / window)
    return sum(scores) / len(scores)


# =========================================================
# 2) Stage 1~6 Analyzer
# =========================================================
@dataclass
class Analyzer:
    verbose: bool = False

    def analyze(self, df: pd.DataFrame, text_features: dict, audio_features: dict, id: int):
        """
        Stage 1~6 ì „ì²´ ìˆ˜í–‰
        merged_df = Cleanerê°€ ë§Œë“¤ì–´ì¤€ í…ìŠ¤íŠ¸+audio_features í¬í•¨ DF
        """

        # Cleanerê°€ ë§Œë“  DF
        merged_df = df.copy()

        # Stage 4(í…ìŠ¤íŠ¸ feature) ì¶”ê°€
        merged_df["text_features"] = merged_df["speaker"].apply(
            lambda s: text_features.get(s)
        )

        # Stage 5(ì˜¤ë””ì˜¤ feature) ì¶”ê°€
        merged_df["audio_features"] = [
        audio_features[i] if isinstance(audio_features, list) and i < len(audio_features)
        else None
        for i in range(len(merged_df))
    ]

        # -------------------------------
        # Stage 2 â€” í…ìŠ¤íŠ¸ Feature
        # -------------------------------
        user_df = merged_df[merged_df["speaker"] == int(id)]
        other_df = merged_df[merged_df["speaker"] != int(id)]

        user_text = " ".join(user_df["text"].tolist())
        other_text = " ".join(other_df["text"].tolist())

        user_words = extract_content_words(user_text)
        other_words = extract_content_words(other_text)

        user_stats = {
            "token_count": len(user_words),
            "mattr": calculate_mattr(user_words),
            "unique_words": len(set(user_words)),
            "top_words": Counter(user_words).most_common(5),
        }

        other_stats = {
            "token_count": len(other_words),
            "mattr": calculate_mattr(other_words),
        }

        statistics = {
            "user": user_stats,
            "others": other_stats
        }

        # -------------------------------
        # â­ NEW â€” Stage 3~4: Prosody Normalization (dialect_normalizer)
        # -------------------------------
        normalizer = DialectProsodyNormalizer()
        prosody_norm = normalizer.normalize(merged_df)

        # -------------------------------
        # â­ NEW â€” Stage 5: Surrogate Context Reasoning
        # -------------------------------
        surrogate = {
            "relationship_pattern": "neutral",
            "emotional_trajectory_hint": "stable",
        }

        # -------------------------------
        # â­ NEW â€” Stage 6: Trigger Detection
        # -------------------------------
        trigger = {
            "trigger_detected": False,
            "intensity": 0.0,
            "emotion_shift": None
        }

        return {
            "statistics": statistics,
            "prosody_norm": prosody_norm,
            "surrogate": surrogate,
            "trigger": trigger,
        }


# =========================================================
# 3) Stage 7 â€” LLM ê¸°ë°˜ ìŠ¤íƒ€ì¼/ê°ì •/ê´€ê³„ ë¶„ì„
# =========================================================
@dataclass
class SafetyLLMAnalyzer:
    def analyze(self, merged_df: pd.DataFrame, id: int,
                stats: Dict[str, Any],
                prosody_norm: Dict[str, Any],
                surrogate: Dict[str, Any],
                trigger: Dict[str, Any]):

        llm = ChatOpenAI(model="gpt-4o-mini")

        user_text = "\n".join(merged_df[merged_df["speaker"] == id]["text"].tolist())
        full_context = "\n".join(merged_df["text"].tolist())

        prompt = f"""
ë‹¤ìŒì€ ì „ì²´ ëŒ€í™” ë‚´ìš©ìž…ë‹ˆë‹¤:
{full_context}

ì•„ëž˜ëŠ” ì‚¬ìš©ìž(ID={id})ì˜ ë°œí™”ë§Œ ëª¨ì€ ë‚´ìš©ìž…ë‹ˆë‹¤:
{user_text}

í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„:
{json.dumps(stats, ensure_ascii=False)}

ìŒí–¥ ê¸°ë°˜ prosody ì •ê·œí™” ì •ë³´:
{json.dumps(prosody_norm, ensure_ascii=False)}

ì¶”ë¡  ê¸°ë°˜ surrogate context:
{json.dumps(surrogate, ensure_ascii=False)}

trigger íƒì§€ ê²°ê³¼:
{json.dumps(trigger, ensure_ascii=False)}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬
ì‚¬ìš©ìžì˜ ë§íˆ¬, ì–µì–‘ íŒ¨í„´, ê°ì • íë¦„, ëŒ€í™” ìŠ¤íƒ€ì¼ì„ JSONìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

í˜•ì‹:
{{
  "tone": "...",
  "prosody": "...",
  "emotion_pattern": "...",
  "strengths": "...",
  "risks": "..."
}}
"""

        resp = llm.invoke(prompt)
        raw = resp.content if hasattr(resp, "content") else str(resp)

        try:
            return json.loads(raw)
        except:
            return {"raw_text": raw}


# =========================================================
# 4) Stage 8 â€” Summary Insight ìƒì„±
# =========================================================
@dataclass
class SummaryBuilder:
    def build(self, user_name: str,
              style: Dict[str, Any],
              statistics: Dict[str, Any],
              prosody_norm: Dict[str, Any]):

        tone = style.get("tone", "íŠ¹ì§• ë¶„ì„ ë¶ˆê°€")
        emotion = style.get("emotion_pattern", "ì •ë³´ ì—†ìŒ")
        mattr = statistics["user"]["mattr"]

        baseline_region = prosody_norm.get("baseline_region", "unknown")

        summary = (
            f"{user_name}ë‹˜ì€ ì´ë²ˆ ëŒ€í™”ì—ì„œ '{tone}' ë§íˆ¬ë¥¼ ë³´ì˜€ìœ¼ë©°, "
            f"ê°ì • íë¦„ì€ '{emotion}' íŒ¨í„´ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. "
            f"MATTR {mattr:.3f} ìˆ˜ì¤€ìœ¼ë¡œ ì–¸ì–´ì  ë‹¤ì–‘ì„±ì€ ì•ˆì •ì ì´ë©°, "
            f"ì–µì–‘ íŒ¨í„´ì€ '{baseline_region}' ì§€ì—­ì˜ íŠ¹ì§•ì— ê°€ìž¥ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
        )

        return summary


# =========================================================
# 5) Stage 9 â€” Temperature Score
# =========================================================
@dataclass
class TemperatureScorer:
    def score(
        self,
        style: Dict[str, Any],
        statistics: Dict[str, Any],
        prosody_norm: Dict[str, Any],
        trigger_info: Dict[str, Any]
    ):
        """
        Warmth Score ê³µì‹ ì ìš©:
        Warmth_Base = 0.30 * Politeness
                    + 0.30 * Empathy
                    + 0.20 * Stability
                    + 0.20 * (1 - Aggressiveness)

        Warmth_Final = Warmth_Base * (1 - 0.4 * Trigger_Intensity)

        Warmth_Score = Warmth_Final * 100 * llm_factor
        """

        # ----- 1) í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì†Œ -----
        politeness = float(style.get("politeness", 0.5))
        empathy = float(style.get("empathy", 0.5))
        aggressiveness = float(style.get("aggressiveness", 0.2))

        # ----- 2) ìŒí–¥ ê¸°ë°˜ ì•ˆì •ì„± -----
        stability = 1.0 - min(abs(prosody_norm.get("prosody_deviation", 0)) / 20, 1)

        # ----- 3) Warmth_Base -----
        warmth_base = (
            0.30 * politeness +
            0.30 * empathy +
            0.20 * stability +
            0.20 * (1 - aggressiveness)
        )

        # ----- 4) Trigger ê°ì  -----
        trigger_intensity = trigger_info.get("intensity", 0.0)
        warmth_after_trigger = warmth_base * (1 - 0.4 * trigger_intensity)

        # ----- 5) LLM ë³´ì • ê³„ìˆ˜ -----
        llm_factor = self._llm_adjust_factor(style, prosody_norm, trigger_info)

        final_score = warmth_after_trigger * 100 * llm_factor

        return round(max(0, min(100, final_score)), 2)

    # =========================================
    # ðŸ”µ NEW â€” LLM ë³´ì • ê³„ìˆ˜ ìƒì„± í•¨ìˆ˜
    # =========================================
    def _llm_adjust_factor(self, style, prosody, trigger):
        llm = ChatOpenAI(model="gpt-4o-mini")

        prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìžì˜ ìŠ¤íƒ€ì¼Â·ê°ì •Â·ìŒí–¥ ì •ë³´ë¥¼ ìš”ì•½í•œ ê²ƒìž…ë‹ˆë‹¤.

ìŠ¤íƒ€ì¼ ë¶„ì„:
{json.dumps(style, ensure_ascii=False)}

ìŒí–¥ ë¶„ì„:
{json.dumps(prosody, ensure_ascii=False)}

íŠ¸ë¦¬ê±° ë¶„ì„:
{json.dumps(trigger, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Warmth Scoreì˜ ë³´ì • ê³„ìˆ˜(0.8~1.2 ì‚¬ì´)ë¥¼ ê²°ì •í•˜ì„¸ìš”.
ìˆ«ìžë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        try:
            resp = llm.invoke(prompt)
            value = float(resp.content.strip())
            return float(min(1.2, max(0.8, value)))
        except:
            return 1.0  # fallback


# =========================================================
# 6) DB ì €ìž¥ Stage
# =========================================================
@dataclass
class AnalysisSaver:
    verbose: bool = False

    def save(self, db: Session, result: Dict[str, Any], state):
        """
        summary / style_analysis / statistics / score ì €ìž¥
        """

        return save_analysis_result(
            db=db,
            id=state.id,
            conv_id=state.conv_id,
            summary=result["summary"],
            style_analysis=result["style_analysis"],
            statistics=result["statistics"],
            score=result["temperature_score"],
            confidence_score=0.0,
            conversation_count=len(state.conversation_df)
        )
