import os
import time
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage
from langsmith import traceable
from app.core.config import settings

# LangSmith í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = settings.langchain_api_key
os.environ["LANGCHAIN_PROJECT"] = settings.langchain_project

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
TEST_PROMPTS = [
    "ê°€ì¡± ê°„ì˜ ì†Œí†µ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ 3ê°€ì§€ ì œì‹œí•´ì£¼ì„¸ìš”.",
    "ë¶€ëª¨ì™€ ìë…€ ê°„ì˜ ëŒ€í™”ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ê°ˆë“± ìƒí™©ê³¼ í•´ê²°ì±…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "íš¨ê³¼ì ì¸ ê²½ì²­ì˜ ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ì‹¤ì œ ì ìš© ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
    "ê°€ì¡± ë‚´ì—ì„œ ê°ì • í‘œí˜„ì„ ì–´ë ¤ì›Œí•˜ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."
]

@traceable(name="gpt4_analysis")
def test_gpt4(prompt: str):
    """GPT-4 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    model = ChatOpenAI(
        model="gpt-4o",
        api_key=settings.openai_api_key,
        temperature=0.7
    )
    
    start_time = time.time()
    response = model.invoke([HumanMessage(content=prompt)])
    end_time = time.time()
    
    return {
        "model": "GPT-4o",
        "response": response.content,
        "response_time": round(end_time - start_time, 2),
        "response_length": len(response.content)
    }

@traceable(name="gpt35_analysis")
def test_gpt35(prompt: str):
    """GPT-3.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    model = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=settings.openai_api_key,
        temperature=0.7
    )
    
    start_time = time.time()
    response = model.invoke([HumanMessage(content=prompt)])
    end_time = time.time()
    
    return {
        "model": "GPT-3.5-turbo",
        "response": response.content,
        "response_time": round(end_time - start_time, 2),
        "response_length": len(response.content)
    }

@traceable(name="gemini_pro_analysis")
def test_gemini_pro(prompt: str):
    """Gemini Pro ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro-latest",
        google_api_key=settings.gemini_api_key,
        temperature=0.7
    )
    
    start_time = time.time()
    response = model.invoke([HumanMessage(content=prompt)])
    end_time = time.time()
    
    return {
        "model": "Gemini-1.5-Pro",
        "response": response.content,
        "response_time": round(end_time - start_time, 2),
        "response_length": len(response.content)
    }

@traceable(name="gemini_flash_analysis")
def test_gemini_flash(prompt: str):
    """Gemini Flash ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=settings.gemini_api_key,
        temperature=0.7
    )
    
    start_time = time.time()
    response = model.invoke([HumanMessage(content=prompt)])
    end_time = time.time()
    
    return {
        "model": "Gemini-1.5-Flash",
        "response": response.content,
        "response_time": round(end_time - start_time, 2),
        "response_length": len(response.content)
    }

@traceable(name="model_comparison_test")
def run_model_comparison():
    """ëª¨ë“  ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print(f"ğŸ“Š LangSmith í”„ë¡œì íŠ¸: {settings.langchain_project}")
    
    models = [test_gpt4, test_gpt35, test_gemini_pro, test_gemini_flash]
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {prompt[:50]}...")
        
        for model_func in models:
            try:
                result = model_func(prompt)
                print(f"âœ… {result['model']}: {result['response_time']}ì´ˆ, {result['response_length']}ì")
            except Exception as e:
                print(f"âŒ {model_func.__name__}: ì˜¤ë¥˜ - {str(e)}")
    
    print(f"\nğŸ¯ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://smith.langchain.com/")
    print(f"ğŸ“ˆ í”„ë¡œì íŠ¸: {settings.langchain_project}")

if __name__ == "__main__":
    run_model_comparison()
