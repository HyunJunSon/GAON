"""
TOC ê¸°ë°˜ ì²­í‚¹ ìœ í‹¸ë¦¬í‹°
rag_test/chunking.py ë¡œì§ í†µí•©
"""
import json
import uuid
import re
import unicodedata
from pathlib import Path
from typing import List, Dict, Tuple, Any
import fitz


class TOCChunker:
    """TOC ê¸°ë°˜ ì²­í‚¹ ì²˜ë¦¬ê¸°"""
    
    # ğŸ“Œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê°•í™”: í”„ë¡¤ë¡œê·¸/ë¨¸ë¦¬ë§/ì²˜ìŒìœ¼ë¡œ/ì¶”ì²œì‚¬/ì—í•„ë¡œê·¸ë„ ì œì™¸
    BLACK_TITLES = {
        "ì°¨ë¡€", "ëª©ì°¨", "íŒê¶Œ", "í‘œì§€",
        "í”„ë¡¤ë¡œê·¸", "ë¨¸ë¦¬ë§", "ì¶”ì²œì‚¬",
        "ì €ì ì†Œê°œ", "ì—¬ëŠ” ë§",
        "ì±…ì˜ ì‹œì‘", "ì²˜ìŒìœ¼ë¡œ",
        "ì—í•„ë¡œê·¸",
    }
    
    # ğŸ“Œ ì •í™• ì¼ì¹˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ì˜¤íƒ ë°©ì§€ìš©)
    NORMALIZE_MAP = {
        "ì €ìì†Œê°œ": "ì €ì ì†Œê°œ",
        "ì—¬ëŠ”ë§": "ì—¬ëŠ” ë§",
    }
    
    def __init__(self, min_chars: int = 600, max_chars: int = 800):
        self.min_chars = min_chars
        self.max_chars = max_chars
    
    def _is_blacklisted(self, title: str) -> bool:
        """ì œëª©ì´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸"""
        if not title:
            return True
            
        # ì •ê·œí™”
        normalized_title = self.NORMALIZE_MAP.get(title.strip(), title.strip())
        
        # ì •í™• ì¼ì¹˜ í™•ì¸
        if normalized_title in self.BLACK_TITLES:
            return True
            
        # ë¶€ë¶„ ì¼ì¹˜ í™•ì¸ (í”„ë¡¤ë¡œê·¸, ë¨¸ë¦¬ë§ ë“±)
        for black_item in self.BLACK_TITLES:
            if black_item in normalized_title:
                return True
                
        return False
    
    def chunk_pdf_by_toc(self, pdf_path: str, toc_data: List[Dict]) -> List[Dict[str, Any]]:
        """TOC ê¸°ë°˜ìœ¼ë¡œ PDF ì²­í‚¹ (ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§ í¬í•¨)"""
        doc = fitz.open(pdf_path)
        
        # ğŸ“Œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§ ì ìš©
        filtered_toc = [entry for entry in toc_data if not self._is_blacklisted(entry.get("title", ""))]
        
        print(f"ğŸ“‹ TOC í•„í„°ë§: {len(toc_data)} â†’ {len(filtered_toc)} (ë¸”ë™ë¦¬ìŠ¤íŠ¸ {len(toc_data) - len(filtered_toc)}ê°œ ì œì™¸)")
        
        # TOC ë°ì´í„°ë¥¼ ê³„ì¸µ êµ¬ì¡°ë¡œ ì •ë¦¬
        parent_index, children_index = self._build_parent_index(filtered_toc)
        
        # ë¦¬í”„ ë…¸ë“œ(ìµœí•˜ìœ„ ëª©ì°¨)ë§Œ ì¶”ì¶œ
        leaf_entries = [entry for entry in filtered_toc if entry["toc_id"] not in children_index]
        
        # ğŸ“Œ ê° ì„¹ì…˜ì˜ ë í˜ì´ì§€ ê³„ì‚°
        for i, entry in enumerate(leaf_entries):
            start_page = entry["page"]
            # ë‹¤ìŒ ì„¹ì…˜ì˜ ì‹œì‘ í˜ì´ì§€ë¥¼ ì°¾ì•„ì„œ ë í˜ì´ì§€ ê²°ì •
            if i + 1 < len(leaf_entries):
                end_page = leaf_entries[i + 1]["page"] - 1
            else:
                end_page = len(doc) - 1  # ë§ˆì§€ë§‰ ì„¹ì…˜ì€ ë¬¸ì„œ ëê¹Œì§€
            
            entry["page_start"] = start_page
            entry["page_end"] = max(start_page, end_page)  # ìµœì†Œí•œ ì‹œì‘ í˜ì´ì§€ì™€ ê°™ê±°ë‚˜ í° ê°’
        
        chunks = []
        for entry in leaf_entries:
            # í•´ë‹¹ ì„¹ì…˜ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            section_text = self._extract_section_text(doc, entry, filtered_toc)
            
            if not section_text.strip():
                continue
            
            # ê³„ì¸µ êµ¬ì¡° ì •ë³´ ìƒì„±
            hierarchy = self._build_hierarchy(entry, parent_index)
            
            # ì²­í‚¹ ìˆ˜í–‰
            section_chunks = self._chunk_text(section_text, entry, hierarchy)
            chunks.extend(section_chunks)
        
        doc.close()
        return chunks
    
    def _build_parent_index(self, toc_data: List[Dict]) -> Tuple[Dict[str, Dict], Dict[str, List[Dict]]]:
        """ë¶€ëª¨-ìì‹ ê´€ê³„ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        parent_index = {}
        children_index = {}
        
        for i, entry in enumerate(toc_data):
            entry_id = entry["toc_id"]
            level = entry["level"]
            
            # ë¶€ëª¨ ì°¾ê¸°
            parent = None
            for j in range(i - 1, -1, -1):
                if toc_data[j]["level"] < level:
                    parent = toc_data[j]
                    break
            
            if parent:
                parent_id = parent["toc_id"]
                parent_index[entry_id] = parent
                
                if parent_id not in children_index:
                    children_index[parent_id] = []
                children_index[parent_id].append(entry)
        
        return parent_index, children_index
    
    def _extract_section_text(self, doc: fitz.Document, entry: Dict, toc_data: List[Dict]) -> str:
        """ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        start_page = entry["page"] - 1  # 0-based
        
        # ë‹¤ìŒ ì„¹ì…˜ì˜ ì‹œì‘ í˜ì´ì§€ ì°¾ê¸°
        end_page = len(doc) - 1
        current_level = entry["level"]
        
        for other in toc_data:
            if (other["page"] > entry["page"] and 
                other["level"] <= current_level):
                end_page = other["page"] - 2  # ì´ì „ í˜ì´ì§€ê¹Œì§€
                break
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_parts = []
        for page_num in range(max(0, start_page), min(len(doc), end_page + 1)):
            page = doc[page_num]
            text = page.get_text()
            if text.strip():
                text_parts.append(text)
        
        return "\n\n".join(text_parts)
    
    def _build_hierarchy(self, entry: Dict, parent_index: Dict[str, Dict]) -> Dict[str, str]:
        """ê³„ì¸µ êµ¬ì¡° ì •ë³´ ìƒì„±"""
        hierarchy = {}
        
        # í˜„ì¬ í•­ëª©ì˜ ë ˆë²¨ì— ë”°ë¼ ì ì ˆí•œ ìœ„ì¹˜ì— ë°°ì¹˜
        current_level = entry["level"]
        level_map = {1: "l1_title", 2: "l2_title", 3: "l3_title"}
        
        # í˜„ì¬ í•­ëª© ë°°ì¹˜
        if current_level in level_map:
            hierarchy[level_map[current_level]] = entry["title"]
        
        # ë¶€ëª¨ë“¤ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•
        current = entry
        while current["toc_id"] in parent_index:
            parent = parent_index[current["toc_id"]]
            parent_level = parent["level"]
            
            if parent_level in level_map:
                hierarchy[level_map[parent_level]] = parent["title"]
            
            current = parent
        
        return hierarchy
    
    def _chunk_text(self, text: str, entry: Dict, hierarchy: Dict[str, str]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        text = self._norm_text(text)
        
        if len(text) <= self.max_chars:
            # ë‹¨ì¼ ì²­í¬
            return [self._create_chunk(text, entry, hierarchy, 0)]
        
        # ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
        chunks = []
        sentences = re.split(r'[.!?]\s+', text)
        
        current_chunk = ""
        chunk_idx = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ì²­í¬ í¬ê¸° í™•ì¸
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(potential_chunk) > self.max_chars and len(current_chunk) >= self.min_chars:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append(self._create_chunk(current_chunk, entry, hierarchy, chunk_idx))
                current_chunk = sentence
                chunk_idx += 1
            else:
                current_chunk = potential_chunk
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk, entry, hierarchy, chunk_idx))
        
        return chunks
    
    def _create_chunk(self, text: str, entry: Dict, hierarchy: Dict[str, str], chunk_idx: int) -> Dict[str, Any]:
        """ì²­í¬ ë°ì´í„° ìƒì„±"""
        # ğŸ“Œ embed_text ìƒì„±: ëŒ€ì œëª©+ì¤‘ì œëª©+ì†Œì œëª©+ë³¸ë¬¸ í˜•ì‹
        title_parts = []
        for level in ["l1_title", "l2_title", "l3_title"]:
            if level in hierarchy and hierarchy[level]:
                title_parts.append(hierarchy[level])
        
        canonical_path = " > ".join(title_parts)
        
        # ğŸ“Œ embed_text í˜•ì‹: [canonical_path] full_text
        embed_text = f"[{canonical_path}] {text}" if canonical_path else text
        
        # ğŸ“Œ book_titleì„ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        book_title = Path(entry.get('book_name', 'Unknown')).stem
        
        return {
            "chunk_id": str(uuid.uuid4()),
            "section_id": entry["toc_id"],
            "canonical_path": canonical_path,
            "chunk_ix": chunk_idx,
            "page_start": entry.get("page_start", entry["page"]),
            "page_end": entry.get("page_end", entry["page"]),
            "full_text": text,
            "embed_text": embed_text.strip(),
            "citation": f"{book_title}, {canonical_path}, p.{entry.get('page_start', entry['page'])}-{entry.get('page_end', entry['page'])}",
            "book_title": book_title,  # ğŸ“Œ íŒŒì¼ëª… ê¸°ë°˜ ì±… ì œëª©
            **hierarchy
        }
    
    def _norm_text(self, s: str, max_len: int = None) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if s is None:
            return ""
        
        s = unicodedata.normalize("NFKC", s)
        s = re.sub(r"\s+", " ", s).strip()
        s = s.replace(""", '"').replace(""", '"').replace("'", "'").replace("'", "'")
        
        if max_len:
            s = s[:max_len]
        
        return s
