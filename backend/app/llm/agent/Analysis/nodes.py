# app/agent/Analysis/nodes.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple
from app.core.config import settings
from langchain_openai import ChatOpenAI
import pandas as pd
from sqlalchemy.orm import Session
from collections import Counter
import re
import json
from kiwipiepy import Kiwi
from app.llm.agent.Analysis.dialect_normalizer import DialectProsodyNormalizer

# âœ… Kiwi ì´ˆê¸°í™”
kiwi = Kiwi()

# =========================================================
# TEXT FEATURE UTILITIES
# =========================================================

def extract_content_words(text: str):
    """Kiwi Token ê°ì²´ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì—¬ ë‚´ìš©ì–´ ì¶”ì¶œ"""
    analyses = kiwi.analyze(text)
    if not analyses:
        return []
    
    morphs = analyses[0][0]   # Token ê°ì²´ ë¦¬ìŠ¤íŠ¸
    content_prefixes = ("NN", "VV", "VA", "MAG", "IC", "NP", "XR", "VX", "SL")
    
    result = []
    for m in morphs:
        if m.tag.startswith(content_prefixes):
            result.append(m.form)
    
    return result


def calculate_mattr(words: List[str], window: int = 25):
    """Moving-Average Type-Token Ratio (MATTR)"""
    if len(words) < window:
        return len(set(words)) / len(words) if words else 0
    
    scores = []
    for i in range(len(words) - window + 1):
        win = words[i:i + window]
        scores.append(len(set(win)) / window)
    
    return sum(scores) / len(scores)


# âœ… CRUD í•¨ìˆ˜ import
from app.llm.agent.crud import (
    get_user_by_id,
    get_family_by_id,
    save_analysis_result,
)


# =========================================
# âœ… UserFetcher (DB ì—°ë™)
# =========================================
@dataclass
class UserFetcher:
    """
    âœ… DBì—ì„œ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
    
    ë³€ê²½ ì‚¬í•­:
    - ê¸°ì¡´: Mock user_df
    - ë³€ê²½: DB users í…Œì´ë¸” ì¡°íšŒ
    """
    def fetch(self, db: Session, conv_state) -> Dict[str, Any]:
        """
        users í…Œì´ë¸”ì—ì„œ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
        
        Args:
            db: SQLAlchemy ì„¸ì…˜
            conv_state: AnalysisState (id í¬í•¨)
        
        Returns:
            ì‚¬ìš©ì ì •ë³´ Dict
        """
        id = conv_state.id
        
        if not id:
            raise ValueError("âŒ UserFetcher: idê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # âœ… DB ì¡°íšŒ
        user = get_user_by_id(db, id)
        
        if not user:
            raise ValueError(f"âŒ UserFetcher: id={id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… [UserFetcher] ì‚¬ìš©ì ì¡°íšŒ: {user.get('user_name')}")
        
        return user


# =========================================
# âœ… FamilyChecker (ê°€ì¡± ê¸°ëŠ¥ ë¹„í™œì„±í™”) - ìˆ˜ì • ì—†ìŒ
# =========================================
@dataclass
class FamilyChecker:
    """
    âœ… ê°€ì¡± ê´€ê³„ í™•ì¸ (í˜„ì¬ ë¹„í™œì„±í™”)
    
    í˜„ì¬ ìƒíƒœ:
    - users â†” family ì—°ê²° ì»¬ëŸ¼ ì—†ìŒ
    - í•­ìƒ False ë°˜í™˜ â†’ LLM ì¶”ë¡  ëª¨ë“œ
    """
    def check(self, db: Session, user_info: Dict[str, Any]) -> Tuple[bool, int]:
        """
        ê°€ì¡± ì •ë³´ í™•ì¸ (í˜„ì¬ ë¹„í™œì„±í™”)
        
        Args:
            db: SQLAlchemy ì„¸ì…˜
            user_info: UserFetcher ê²°ê³¼
        
        Returns:
            (False, None) - í•­ìƒ LLM ì¶”ë¡  ëª¨ë“œ
        """
        print(f"âš ï¸  [FamilyChecker] ê°€ì¡± ê¸°ëŠ¥ ë¹„í™œì„±í™” â†’ LLM ì¶”ë¡  ëª¨ë“œ")
        return False, None


# =========================================
# âœ… RelationResolver_DB (ë¹„í™œì„±í™”)
# =========================================
@dataclass
class RelationResolver_DB:
    """
    âœ… DBì—ì„œ ê°€ì¡± êµ¬ì„±ì› ì¡°íšŒ (í˜„ì¬ ë¹„í™œì„±í™”)
    
    í˜„ì¬ ìƒíƒœ:
    - family_member í…Œì´ë¸” ì—†ìŒ
    - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    def resolve(self, db: Session, fam_id: int) -> List[Dict[str, Any]]:
        """
        ê°€ì¡± êµ¬ì„±ì› ì¡°íšŒ (í˜„ì¬ ë¹„í™œì„±í™”)
        
        Args:
            db: SQLAlchemy ì„¸ì…˜
            fam_id: ê°€ì¡± ID
        
        Returns:
            [] - ë¹ˆ ë¦¬ìŠ¤íŠ¸
        """
        print(f"âš ï¸  [RelationResolver_DB] ê°€ì¡± ê¸°ëŠ¥ ë¹„í™œì„±í™”")
        return []


# =========================================
# âœ… RelationResolver_LLM 
# =========================================
@dataclass
class RelationResolver_LLM:
    """LLM ê¸°ë°˜ ê´€ê³„ ì¶”ë¡ """
    verbose: bool = False

    def resolve(self, conversation_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        LLMìœ¼ë¡œ ëŒ€í™”ì—ì„œ ê´€ê³„ ì¶”ë¡ 
        
        Args:
            conversation_df: ëŒ€í™” DataFrame
        
        Returns:
            ì¶”ë¡ ëœ ê´€ê³„ ë¦¬ìŠ¤íŠ¸
        """
        llm = ChatOpenAI(model="gpt-4o-mini", api_key=settings.openai_api_key)
        text_snippet = "\n".join(conversation_df["text"].tolist()[:10])
        
        prompt = f"""
ë‹¤ìŒ ëŒ€í™”ì—ì„œ ë“±ì¥í•˜ëŠ” ì¸ë¬¼ë“¤ì˜ ê´€ê³„ë¥¼ ì¶”ë¡ í•´ì¤˜.
ì˜ˆ: ì—„ë§ˆ, ì•„ë“¤, ì•„ë¹ , ì¹œêµ¬ ë“±

ëŒ€í™” ë‚´ìš©:
{text_snippet}

ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•´ì¤˜.
ì˜ˆ: [{{"speaker":1,"relation":"ì—„ë§ˆ"}}, {{"speaker":2,"relation":"ì•„ë“¤"}}]
speakerëŠ” ë°˜ë“œì‹œ int í˜•íƒœë¡œ ë°˜í™˜í•´ì•¼í•´.
"""
        
        try:
            response = llm.invoke(prompt)
            content = response.content if hasattr(response, "content") else str(response)
            
            if self.verbose:
                print(f"ğŸ§  [RelationResolver_LLM] ì‘ë‹µ: {content[:200]}")
            
            # âœ… ê°„ë‹¨í•œ fallback
            return [
                {"speaker": 1, "relation": "ì°¸ì„ì1"},
                {"speaker": 2, "relation": "ì°¸ì„ì2"}
            ]
            
        except Exception as e:
            print(f"âš ï¸ Relation LLM ì‹¤íŒ¨: {e}")
            return []


# =========================================
# ğŸ”§ Analyzer (ì‚¬ìš©ì ì¤‘ì‹¬ ë¶„ì„)
# =========================================

@dataclass
@dataclass
class Analyzer:
    verbose: bool = False

    def analyze(
        self,
        speaker_segments: List[Dict[str, Any]],
        user_id: int,
        user_gender: str,
        user_age: int,
        user_name: str,
        user_speaker_label: str,
        other_speaker_label: str,
        other_display_name: str
    ):

        # ----------------------------------
        # 1) DataFrame ìƒì„±
        # ----------------------------------
        df = pd.DataFrame([{
            "speaker": seg["speaker"],
            "text": seg["text"]
        } for seg in speaker_segments])

        # ğŸ” DEBUG â€” DF ì „ì²´ ì¶œë ¥
        print("\n[DEBUG] DF created:")
        print(df.head(10))

        # ----------------------------------
        # 2) í…ìŠ¤íŠ¸ Feature
        # ----------------------------------
        user_df = df[df["speaker"] == user_speaker_label]
        other_df = df[df["speaker"] != user_speaker_label]

        print("\n[DEBUG] user_df:", user_df.head())
        print("[DEBUG] other_df:", other_df.head())

        user_text = " ".join(user_df["text"].tolist())
        other_text = " ".join(other_df["text"].tolist())

        print("\n[DEBUG] user_text:", user_text)
        print("[DEBUG] other_text:", other_text)

        user_words = extract_content_words(user_text)
        other_words = extract_content_words(other_text)

        print("\n[DEBUG] user_words:", user_words)
        print("[DEBUG] other_words:", other_words)

        statistics = {
            "user": {
                "token_count": len(user_words),
                "mattr": calculate_mattr(user_words),
                "unique_words": len(set(user_words)),
                "top_words": Counter(user_words).most_common(5),
            },
            "others": {
                "token_count": len(other_words),
                "mattr": calculate_mattr(other_words),
            }
        }

        # ----------------------------------
        # 3) Prosody Normalization
        # ----------------------------------
        normalizer = DialectProsodyNormalizer()
        prosody_norm = normalizer.normalize(speaker_segments)

        # ----------------------------------
        # 4) Surrogate (ê°ì • íë¦„ + ë°˜ì‘ì„±)
        # ----------------------------------
# (1) ê°ì • íë¦„ ë¶„ì„: prosody_normì˜ slope ê¸°ë°˜
        slopes = [
            t.get("observed_slope")
            for t in prosody_norm.get("turn_prosody", [])
            if t.get("observed_slope") is not None
        ]

        if slopes:
            avg_slope = sum(slopes) / len(slopes)
            if avg_slope > 5:
                emotion_trajectory = "rising (ê°ì • ìƒìŠ¹)"
            elif avg_slope < -5:
                emotion_trajectory = "falling (ê°ì • í•˜ê°•)"
            else:
                emotion_trajectory = "stable (ì•ˆì •ì )"
        else:
            emotion_trajectory = "unknown"

        # (2) ë°˜ì‘ì„± ë¶„ì„: ë°œí™” ê°„ í…€(ì‹œê°„), ë°œí™” ê¸¸ì´ ê¸°ë°˜
        from numpy import mean

        durations = []
        for seg in speaker_segments:
            start = seg.get("start")
            end = seg.get("end")
            if start is not None and end is not None:
                durations.append(end - start)

        if durations:
            avg_len = mean(durations)
            if avg_len > 5:
                responsiveness = "slow"
            elif avg_len < 1.5:
                responsiveness = "fast"
            else:
                responsiveness = "moderate"
        else:
            responsiveness = "unknown"

        # (3) ë°œí™” ë¹„ìœ¨ ê¸°ë°˜ ì£¼ë„ì„±
        user_count = len(user_df)
        other_count = len(other_df)
        dominance_ratio = user_count / (user_count + other_count + 1e-6)

        if dominance_ratio > 0.65:
            dominance = "high"
        elif dominance_ratio < 0.35:
            dominance = "low"
        else:
            dominance = "balanced"

        # ìµœì¢… Surrogate êµ¬ì„±
        surrogate = {
            "emotion_trajectory": emotion_trajectory,
            "responsiveness": responsiveness,
            "dominance": dominance,
            "relationship_pattern": "neutral",  # ê¸°ë³¸ê°’ ìœ ì§€
        }

        # ----------------------------------
        # 5) Trigger Detection
        # ----------------------------------
        trigger = self._detect_triggers(speaker_segments, prosody_norm)

        # ----------------------------------
        # 6) LLM Style Analysis
        # ----------------------------------
        style_analyzer = SafetyLLMAnalyzer()
        style = style_analyzer.analyze(
            df=df,
            user_speaker_label=user_speaker_label,
            user_gender=user_gender,
            user_age=user_age,
            stats=statistics,
            prosody_norm=prosody_norm,
            surrogate=surrogate,
            trigger=trigger
        )

        # ----------------------------------
        # 7) Temperature Score
        # ----------------------------------
        scorer = TemperatureScorer()
        score = scorer.score(style, prosody_norm, trigger)

        # ----------------------------------
        # 8) Summary
        # ----------------------------------
        summary_builder = SummaryBuilder()
        summary = summary_builder.build(
            user_name=user_name if user_name else "ì‚¬ìš©ì",
            df=df,
            user_speaker_label=user_speaker_label,
            user_gender=user_gender,
            user_age=user_age,
            style=style,
            statistics=statistics,
            prosody_norm=prosody_norm,
            surrogate=surrogate,
            trigger=trigger
        )

        return {
            "statistics": statistics,
            "prosody_norm": prosody_norm,
            "surrogate": surrogate,
            "trigger": trigger,
            "style": style,
            "score": score,
            "summary": summary,
            "df": df,
        }


    # =========================================================
    # Trigger (deviation ê¸°ë°˜ rule)
    # =========================================================
    def _detect_triggers(self, segments, prosody_norm):
        deviations = [
            r.get("emotional_deviation", 0)
            for r in prosody_norm.get("turn_prosody", [])
            if r.get("emotional_deviation") is not None
        ]

        if not deviations:
            return {"trigger_detected": False, "intensity": 0.0, "emotion_shift": None}

        max_dev = max(abs(d) for d in deviations)

        trigger_detected = max_dev > 20
        intensity = min(max_dev / 40, 1)

        return {
            "trigger_detected": trigger_detected,
            "intensity": round(float(intensity), 3),
            "emotion_shift": "abrupt_change" if trigger_detected else "stable"
        }


# =========================================================
# 2) Stage 7 â€” LLM STYLE ANALYZER
# =========================================================

class ScoreEvaluator:
    """ì‹ ë¢°ë„ í‰ê°€"""
    def evaluate(self, result: Dict[str, Any]) -> bool:
        """
        ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ í‰ê°€
        
        Args:
            result: Analyzer ê²°ê³¼
        
        Returns:
            ì‹ ë¢°ë„ >= 0.65 ì—¬ë¶€
        """
        score = result.get("score", 0)
        return score >= 0.65


# =========================================
# ğŸ”§ AnalysisSaver
# =========================================
@dataclass
class AnalysisSaver:
    """
    âœ… DBì— ë¶„ì„ ê²°ê³¼ ì €ì¥
    
    ğŸ”§ ìˆ˜ì • ì‚¬í•­:
    - statistics ì €ì¥ (ë¹ˆ dict â†’ ì‹¤ì œ ë°ì´í„°)
    """
    verbose: bool = False  # ğŸ”§ ì¶”ê°€
    
    def save(self, db: Session, result: Dict[str, Any], state) -> Dict[str, Any]:
        """
        analysis_result í…Œì´ë¸”ì— INSERT
        
        Args:
            db: SQLAlchemy ì„¸ì…˜
            result: Analyzer ê²°ê³¼
            state: AnalysisState
        
        Returns:
            ì €ì¥ ê²°ê³¼
        """
        if not result:
            return {"status": "no_result"}
        
        try:
            print("ğŸ’¾ [DEBUG] AnalysisSaver.save() ì§„ì…")
            print(f"ğŸ’¾ state.id={state.id}, conv_id={state.conv_id}")
            print(f"ğŸ’¾ result keys={list(result.keys()) if result else None}")


            saved = save_analysis_result(
                db=db,
                id=str(state.id),
                conv_id=str(state.conv_id),
                summary=result.get("summary", ""),
                style_analysis=result.get("style_analysis", {}),
                statistics=result.get("statistics", {}),  # â† ğŸ”§ ìˆ˜ì •
                score=result.get("score", 0.0),
                confidence_score=0.0,  # QAì—ì„œ ì—…ë°ì´íŠ¸
                conversation_count=len(state.conversation_df) if state.conversation_df is not None else 0,
                feedback=None,
            )
            
            print(f"âœ… [AnalysisSaver] DB ì €ì¥ ì™„ë£Œ: analysis_id={saved['analysis_id']}")
            
            # ğŸ”§ ì¶”ê°€: ì €ì¥ëœ ë°ì´í„° ìƒì„¸ ì¶œë ¥
            if self.verbose:
                print(f"   â†’ summary: {result.get('summary', '')[:50]}...")
                print(f"   â†’ score: {result.get('score', 0):.2f}")
                
                # statistics í™•ì¸
                stats = result.get("statistics", {})
                if stats:
                    user_stats = stats.get("user", {})
                    print(f"   â†’ ì‚¬ìš©ì ë‹¨ì–´ ìˆ˜: {user_stats.get('token_count', 0)}")
                    print(f"   â†’ ì‚¬ìš©ì í‰ê·  ë¬¸ì¥ ê¸¸ì´: {user_stats.get('avg_sentence_length', 0)}")
            
            # âœ… stateì— ì €ì¥
            state.meta["analysis_id"] = saved["analysis_id"]
            
            return {
                "status": "saved",
                "analysis_id": saved["analysis_id"],
            }
            
        except Exception as e:
            print(f"âŒ [AnalysisSaver] ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "error": str(e)}

# =========================================================
# SafetyLLMAnalyzer - LLM ê¸°ë°˜ ìŠ¤íƒ€ì¼ ë¶„ì„
# =========================================================
@dataclass
class SafetyLLMAnalyzer:
    def analyze(
        self,
        df: pd.DataFrame,
        user_speaker_label: str,
        user_gender: str,
        user_age: int,
        stats: Dict[str, Any],
        prosody_norm: Dict[str, Any],
        surrogate: Dict[str, Any],
        trigger: Dict[str, Any]
    ):
        llm = ChatOpenAI(model="gpt-4o-mini", api_key=settings.openai_api_key)

        user_text = "\n".join(df[df["speaker"] == user_speaker_label]["text"].tolist())
        full_context = "\n".join(df["text"].tolist())

        prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í™” ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”.
ë‹¤ìŒì€ ì „ì²´ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì „ì²´ ë°œí™”ì—ì„œ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•œ í›„, ë§¥ë½ì— ê·¼ê±°í•˜ì—¬ ì‚¬ìš©ì ì •ë³´ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ í™•ì¸í•˜ê³  ì‚¬ìš©ìì˜ ë°œí™” ìŠ¤íƒ€ì¼ì„ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

{full_context}

ì‚¬ìš©ì({user_speaker_label}) ë°œí™”ë§Œ:
{user_text}

ì‚¬ìš©ì ì •ë³´:
- ë‚˜ì´: {user_age}
- ì„±ë³„: {user_gender}

í…ìŠ¤íŠ¸ í†µê³„:
{json.dumps(stats, ensure_ascii=False)}

ìŒí–¥Â·ì–µì–‘ ë¶„ì„:
{json.dumps(prosody_norm, ensure_ascii=False)}

ë§¥ë½ íŒíŠ¸:
{json.dumps(surrogate, ensure_ascii=False)}

íŠ¸ë¦¬ê±° ì •ë³´:
{json.dumps(trigger, ensure_ascii=False)}

ë‹¤ìŒì„ JSONìœ¼ë¡œ ì¶œë ¥:
{{
  "tone": "...",
  "prosody": "...",
  "emotion_pattern": "...",
  "strengths": "...",
  "risks": "...",
  "politeness": 0.0,
  "empathy": 0.0,
  "aggressiveness": 0.0
}}
"""

        resp = llm.invoke(prompt)
        raw = resp.content if hasattr(resp, "content") else str(resp)

        try:
            return json.loads(raw)
        except:
            return {"raw_text": raw}


# =========================================================
# SummaryBuilder - LLM ê¸°ë°˜ ìš”ì•½ ìƒì„±
# =========================================================
@dataclass
class SummaryBuilder:
    def build(
        self,
        user_name: str,
        df: pd.DataFrame,
        user_speaker_label: str,
        user_gender: str,
        user_age: int,
        style: Dict[str, Any],
        statistics: Dict[str, Any],
        prosody_norm: Dict[str, Any],
        surrogate: Dict[str, Any],
        trigger: Dict[str, Any],
    ):
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=settings.openai_api_key)
        full_context = "\n".join(df["text"].tolist())
        user_text = "\n".join(df[df["speaker"] == user_speaker_label]["text"].tolist())

        prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í™” ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì „ì²´ ë°œí™”, {user_name}ë‹˜ ì •ë³´, í…ìŠ¤íŠ¸Â·ìŒí–¥ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
**ì •í•´ì§„ ë¦¬í¬íŠ¸ í˜•ì‹ ê·¸ëŒ€ë¡œ** ê³ ê¸‰ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
ë³´ê³ ì„œëŠ” ë°˜ë“œì‹œ 1200ì ì´ìƒì´ë©°, JSONì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

âš ï¸ ì¤‘ìš”:
- ì•„ë˜ êµ¬ì¡°, ì œëª©, êµ¬ë¶„ì„  ëª¨ë‘ ê·¸ëŒ€ë¡œ ì¶œë ¥
- "ì‚¬ìš©ì"ë¼ëŠ” ë‹¨ì–´ë¥¼ ì“°ì§€ ë§ê³  ë°˜ë“œì‹œ **"{user_name}ë‹˜"**ì´ë¼ê³  ì§€ì¹­
- ë°ì´í„° ë‚˜ì—´ì´ ì•„ë‹Œ â€˜í•´ì„ ì¤‘ì‹¬â€™ìœ¼ë¡œ ì‘ì„±
- ë‹¨ì •ì  í‰ê°€ ê¸ˆì§€ (ê´€ì°° ê¸°ë°˜ ì„œìˆ )

============================================================================
ğŸ“Š ëŒ€í™” ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸
============================================================================

[ë¶„ì„ ëŒ€ìƒ] {user_name}ë‹˜  
[ëŒ€í™” ê·œëª¨] ì „ì²´ ë°œí™” ìˆ˜: {len(df)}íšŒ  
({user_name}ë‹˜ ë°œí™”: {len(df[df["speaker"] == user_speaker_label])}íšŒ,  
ìƒëŒ€ë°© ë°œí™”: {len(df[df["speaker"] != user_speaker_label])}íšŒ)

----------------------------------------------------------------------------
ğŸ¯ ëŒ€í™”ì˜ ì˜¨ë„: (prosody_norm ê¸°ë°˜ ì •ì„œì  ì•ˆì •ë„ í•´ì„)
----------------------------------------------------------------------------

ğŸ“ˆ í†µê³„ ë¶„ì„  
â€¢ {user_name}ë‹˜ ì´ ë‹¨ì–´ ìˆ˜: {statistics.get("token_count_user")}  
â€¢ í‰ê·  ë¬¸ì¥ ê¸¸ì´: {statistics.get("avg_sentence_len_user")}ë‹¨ì–´  
â€¢ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {statistics.get("unique_words_user")}  

â€¢ ìƒëŒ€ë°© ì´ ë‹¨ì–´ ìˆ˜: {statistics.get("token_count_other")}  
â€¢ ìƒëŒ€ë°© í‰ê·  ë¬¸ì¥ ê¸¸ì´: {statistics.get("avg_sentence_len_other")}ë‹¨ì–´  

â€¢ ë¹„êµ ë¶„ì„: {user_name}ë‹˜ì˜ ë°œí™”ëŸ‰ê³¼ ìƒëŒ€ë°© ë°œí™”ëŸ‰ì˜ ì°¨ì´ ë° íŒ¨í„´ ì„¤ëª…

ğŸ¤– AI í•´ì„:  
ìœ„ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {user_name}ë‹˜ì˜ í‘œí˜„ ë°©ì‹, ì–´íœ˜ ë‹¤ì–‘ì„±(MATTR í¬í•¨), ë¬¸ì¥ êµ¬ì„± ìŠµê´€ì„  
6~7ë¬¸ì¥ ì´ìƒ ì „ë¬¸ê°€ ê´€ì ì—ì„œ í•´ì„í•´ ì„œìˆ í•˜ì„¸ìš”.

----------------------------------------------------------------------------
ğŸ—£ï¸ ë§íˆ¬ íŠ¹ì§• ë¶„ì„
----------------------------------------------------------------------------

{user_name}ë‹˜ì˜ ë§íˆ¬/ì–¸ì–´ ìŠ¤íƒ€ì¼ì„ ì•„ë˜ ìš”ì†Œ ì¤‘ì‹¬ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…:
- ë§íˆ¬ì˜ ì „ì²´ ë¶„ìœ„ê¸°  
- ì¡´ëŒ“ë§/ë°˜ë§ ì‚¬ìš© ê²½í–¥  
- ê°„ê²°ì„±Â·ì§ì„¤ì„±Â·ì™„ê³¡ì„±  
- prosody_norm ê¸°ë°˜ ì–µì–‘Â·ì†ë„Â·ë¦¬ë“¬  
- ë°˜ë³µë˜ëŠ” íŒ¨í„´ ë˜ëŠ” íŠ¹ì§•  

----------------------------------------------------------------------------
ğŸ’¬ ëŒ€í™” ì„±í–¥ ë° ê°ì • í‘œí˜„
----------------------------------------------------------------------------

ì „ì²´ ë§¥ë½(full_context)ì—ì„œ {user_name}ë‹˜ì˜ ê°ì • íë¦„ê³¼ í‘œí˜„ ë°©ì‹ì„  
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 10ë¬¸ì¥ ì´ìƒ í•´ì„:
- ê°ì •ì´ ë“œëŸ¬ë‚˜ëŠ” ìˆœê°„  
- ê°ì • í‘œí˜„ì˜ ê¹Šì´/ë°©ì‹  
- ì •ì„œì  ì•ˆì •ê° ë˜ëŠ” ì·¨ì•½ì„±  
- íŠ¹ì • ì£¼ì œì—ì„œì˜ ë°˜ì‘ì„± (trigger ê¸°ë°˜)  
- ìƒëŒ€ë°©ê³¼ì˜ ê´€ê³„ì  íë¦„ (surrogate ê¸°ë°˜)  
- ë§í•˜ê¸°ë¥¼ í†µí•´ ë“œëŸ¬ë‚˜ëŠ” ë‚´ì  ìƒíƒœ  

----------------------------------------------------------------------------
ğŸ¯ ì£¼ìš” ê´€ì‹¬ì‚¬
----------------------------------------------------------------------------

ì „ì²´ ë°œí™”ì—ì„œ {user_name}ë‹˜ì´ ë°˜ë³µì ìœ¼ë¡œ ë“œëŸ¬ë‚¸ ê´€ì‹¬ì‚¬Â·ê°€ì¹˜Â·ì£¼ìš” ì£¼ì œë¥¼  
5~8ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.

----------------------------------------------------------------------------
ğŸ“Š ìƒëŒ€ë°©ê³¼ì˜ ë¹„êµ
----------------------------------------------------------------------------

{user_name}ë‹˜ê³¼ ìƒëŒ€ë°©ì˜ ëŒ€í™” ìŠ¤íƒ€ì¼ ì°¨ì´ë¥¼  
ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ 10ë¬¸ì¥ ì´ìƒ ë¶„ì„:
- ë¬¸ì¥ ê¸¸ì´  
- ë§í•˜ê¸° ë¹„ì¤‘  
- ëŒ€í™” ì£¼ë„ì„±  
- ì§ˆë¬¸Â·ì‘ë‹µ íŒ¨í„´  
- ê°ì • í‘œí˜„ ë°©ì‹  
- ìƒí˜¸ì‘ìš© ë¦¬ë“¬ì˜ ì°¨ì´  

----------------------------------------------------------------------------
ğŸ¤– AI ì¢…í•© ë¶„ì„
----------------------------------------------------------------------------

{user_name}ë‹˜ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê°•ì Â·íŠ¹ì§•Â·ì •ì„œì  ìì›  
ê·¸ë¦¬ê³  ê´€ê³„ ê°œì„  ë˜ëŠ” í‘œí˜„ í™•ì¥ ì¸¡ë©´ì—ì„œì˜ ê°€ëŠ¥ì„±ì„  
ì „ë¬¸ê°€ ì½”ì¹­ ìŠ¤íƒ€ì¼ë¡œ 12ë¬¸ì¥ ì´ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.

============================================================================

ğŸ“Œ ì¶”ê°€ ì‘ì„± ê·œì¹™:
- bulletì€ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ ì „ì²´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì„œìˆ í˜• ë³´ê³ ì„œì¼ ê²ƒ
- ë°ì´í„° ê°’ì€ ë‚˜ì—´ì´ ì•„ë‹ˆë¼ ë°˜ë“œì‹œ ì˜ë¯¸ë¥¼ í•´ì„í•´ ì„œìˆ 
- JSON ì¶œë ¥ ê¸ˆì§€
- ì „ì²´ ë¶„ëŸ‰ 1200ì ì´ìƒ í•„ìˆ˜
- "ì‚¬ìš©ì"ë¼ëŠ” ë‹¨ì–´ ì‚¬ìš© ê¸ˆì§€ â†’ ë°˜ë“œì‹œ **{user_name}ë‹˜**ìœ¼ë¡œ ì§€ì¹­

============================================================================

# ì „ì²´ ëŒ€í™” ë‚´ìš©
{full_context}

# {user_name}ë‹˜ ë°œí™”({user_speaker_label})
{user_text}

# í…ìŠ¤íŠ¸ í†µê³„
{json.dumps(statistics, ensure_ascii=False, indent=2)}

# Prosody ë¶„ì„
{json.dumps(prosody_norm, ensure_ascii=False, indent=2)}

# ìŠ¤íƒ€ì¼ ë¶„ì„ ê²°ê³¼
{json.dumps(style, ensure_ascii=False, indent=2)}

# Surrogate ì •ë³´
{json.dumps(surrogate, ensure_ascii=False, indent=2)}

# Trigger ì •ë³´
{json.dumps(trigger, ensure_ascii=False, indent=2)}
"""


        resp = llm.invoke(prompt)
        summary = resp.content if hasattr(resp, "content") else str(resp)
        return summary.strip()


# =========================================================
# TemperatureScorer - ì˜¨ë„ ì ìˆ˜ ê³„ì‚°
# =========================================================
@dataclass
class TemperatureScorer:
    def score(self, style, prosody_norm, trigger):
        politeness = float(style.get("politeness", 0.5))
        empathy = float(style.get("empathy", 0.5))
        aggressiveness = float(style.get("aggressiveness", 0.2))

        deviation = prosody_norm.get("mean_observed_slope", 0)
        stability = 1.0 - min(abs(deviation) / 60, 1)

        warmth_base = (
            0.35 * politeness +
            0.35 * empathy +
            0.15 * stability +
            0.15 * (1 - aggressiveness)
        )

        intensity = trigger.get("intensity", 0.0)
        warmth_after_trigger = warmth_base * (1 - 0.2 * intensity)

        final_score = max(30, warmth_after_trigger * 100)

        return round(min(100, final_score), 2)
