import os
import uuid
from typing import BinaryIO, Tuple, List, Dict, Any
from google.cloud import storage
from pypdf import PdfReader
from docx import Document
import io


class FileProcessor:
    def __init__(self, bucket_name: str = "gaon-cloud-data"):
        self.bucket_name = bucket_name
        self.base_path = "user-upload-conv-data"  # ê¸°ë³¸ ê²½ë¡œ
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket_name)

    def validate_file_content(self, file_content: bytes, file_type: str) -> bool:
        """íŒŒì¼ ë‚´ìš© ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            if file_type == "pdf":
                PdfReader(io.BytesIO(file_content))
            elif file_type == "docx":
                Document(io.BytesIO(file_content))
            elif file_type == "txt":
                file_content.decode('utf-8')
            return True
        except Exception:
            return False

    def extract_text(self, file_content: bytes, file_type: str) -> str:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            if file_type == "txt":
                return file_content.decode('utf-8')
            
            elif file_type == "pdf":
                reader = PdfReader(io.BytesIO(file_content))
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text.strip()
            
            elif file_type == "docx":
                doc = Document(io.BytesIO(file_content))
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text.strip()
            
            return ""
        except Exception as e:
            raise ValueError(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text:
            return []
        
        chunks = []
        start = 0
        chunk_id = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ì°¾ê¸°)
            if end < len(text):
                last_sentence_end = max(
                    chunk_text.rfind('.'),
                    chunk_text.rfind('!'),
                    chunk_text.rfind('?'),
                    chunk_text.rfind('\n')
                )
                if last_sentence_end > chunk_size // 2:  # ì²­í¬ì˜ ì ˆë°˜ ì´ìƒì—ì„œ ë°œê²¬ëœ ê²½ìš°ë§Œ
                    chunk_text = chunk_text[:last_sentence_end + 1]
                    end = start + last_sentence_end + 1
            
            chunks.append({
                "chunk_id": chunk_id,
                "text": chunk_text.strip(),
                "start_pos": start,
                "end_pos": end,
                "length": len(chunk_text.strip())
            })
            
            chunk_id += 1
            start = end - overlap  # ì˜¤ë²„ë© ì ìš©
            
            if start >= len(text):
                break
        
        return chunks

    def upload_to_gcs(self, file_content: bytes, user_id: int, original_filename: str) -> str:
        """GCSì— íŒŒì¼ ì—…ë¡œë“œ (ì†ë„ ìµœì í™” ì ìš©)"""
        try:
            # ê³ ìœ í•œ íŒŒì¼ ê²½ë¡œ ìƒì„±
            file_id = str(uuid.uuid4())
            file_extension = original_filename.split('.')[-1].lower()
            # ê²½ë¡œ: user-upload-conv-data/conversations/user_123/uuid.txt
            gcs_path = f"{self.base_path}/conversations/user_{user_id}/{file_id}.{file_extension}"
            
            # GCSì— ì—…ë¡œë“œ (í´ë” êµ¬ì¡°ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë¨)
            blob = self.bucket.blob(gcs_path)
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ Content-Type ì„¤ì •
            content_type_map = {
                'webm': 'audio/webm',
                'wav': 'audio/wav',
                'mp3': 'audio/mpeg',
                'm4a': 'audio/mp4',
                'txt': 'text/plain',
                'pdf': 'application/pdf',
                'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            }
            content_type = content_type_map.get(file_extension, 'application/octet-stream')
            
            # ğŸš€ ì†ë„ ìµœì í™” ì„¤ì •
            # 1. ì²­í¬ í¬ê¸° ìµœì í™” (256KB - 8MB ê¶Œì¥, ê¸°ë³¸ê°’ë³´ë‹¤ í° ê°’)
            blob.chunk_size = 1024 * 1024 * 2  # 2MB chunks (ê¸°ë³¸ê°’ ëŒ€ë¹„ í–¥ìƒ)
            
            # 2. í° íŒŒì¼ì˜ ê²½ìš° resumable upload í™œì„±í™” (ìë™)
            # 3. ì••ì¶• ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ê²½ìš° gzip ì••ì¶•
            if file_extension in ['txt', 'json', 'csv']:
                import gzip
                compressed_content = gzip.compress(file_content)
                blob.content_encoding = 'gzip'
                blob.upload_from_string(compressed_content, content_type=content_type)
            else:
                blob.upload_from_string(file_content, content_type=content_type)
            
            return gcs_path
        except Exception as e:
            raise ValueError(f"GCS ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def delete_from_gcs(self, gcs_path: str) -> bool:
        """GCSì—ì„œ íŒŒì¼ ì‚­ì œ"""
        try:
            blob = self.bucket.blob(gcs_path)
            blob.delete()
            return True
        except Exception:
            return False