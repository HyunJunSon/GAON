# -*- coding: utf-8 -*-
# summary_rag.py
# - analysis_idê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ìš”ì•½ìœ¼ë¡œ, ì—†ìœ¼ë©´ created_at ìµœì‹  ìš”ì•½ìœ¼ë¡œ
# - ìš”ì•½ ì„ë² ë”© â†’ pgvector KNN â†’ section ìŠ¤í‹°ì¹­ â†’ í”„ë¦¬ë·°

import os, sys, json
import psycopg2
import psycopg2.extras as extras
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# ----- .env ë¡œë”© (í˜„ì¬ í´ë”/ìƒìœ„ í´ë” ìš°ì„  íƒìƒ‰) -----
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

# ----- DB helpers -----
def fetch_analysis_by_id(conn, analysis_id: str) -> dict:
    sql = "SELECT * FROM analysis_result WHERE analysis_id = %s LIMIT 1"
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (analysis_id,))
        row = cur.fetchone()
    if not row:
        return {}
    for k in ("statistics", "style_analysis"):
        if isinstance(row.get(k), str):
            try: row[k] = json.loads(row[k])
            except: pass
    return row

def fetch_latest_analysis_id_by_created(conn, user_id: str|None=None, conv_id: str|None=None) -> str|None:
    base = "SELECT analysis_id FROM analysis_result"
    where, params = [], []
    if user_id:
        where.append("user_id = %s"); params.append(user_id)
    if conv_id:
        where.append("conv_id = %s"); params.append(conv_id)
    where_sql = (" WHERE " + " AND ".join(where)) if where else ""
    sql = f"""{base}{where_sql}
              ORDER BY created_at DESC NULLS LAST
              LIMIT 1"""
    with conn.cursor() as cur:
        cur.execute(sql, params)
        row = cur.fetchone()
    return row[0] if row else None

def fetch_full_sections(conn, table: str, section_ids: list[str]) -> list[dict]:
    """
    ì£¼ì–´ì§„ section_id ë“¤ì— ëŒ€í•´, ì„¹ì…˜ì— ì†í•œ ëª¨ë“  ìŠ¤ë‹ˆí«ì„ ê°€ì ¸ì™€
    chunk_ix / page ìˆœìœ¼ë¡œ ì •ë ¬ â†’ ì „ì²´ ë³¸ë¬¸/ì¸ìš©ì„ ì¬ì¡°ë¦½.
    """
    if not section_ids:
        return []

    sql = f"""
      SELECT section_id, canonical_path, chunk_ix,
             page_start, page_end, citation, full_text
      FROM {table}
      WHERE section_id = ANY(%s)
      ORDER BY section_id, chunk_ix, page_start, page_end
    """
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (section_ids,))
        rows = cur.fetchall()

    by = {}
    for r in rows:
        g = by.setdefault(r["section_id"], {
            "section_id": r["section_id"],
            "canonical_path": r["canonical_path"],
            "snippets": [],
            "citations": set(),
        })
        g["snippets"].append(r)
        if r.get("citation"):
            g["citations"].add(r["citation"])

    sections = []
    for sec in by.values():
        text = "\n\n".join(x["full_text"] for x in sec["snippets"] if x.get("full_text"))
        sections.append({
            "section_id": sec["section_id"],
            "canonical_path": sec["canonical_path"],
            "text": text.strip(),
            "citations": sorted(sec["citations"]),
        })
    return sections

# ----- Embedding / KNN / Stitch -----
def make_query_embedding(client: OpenAI, text: str, model="text-embedding-3-small") -> list:
    t = (text or "").strip()
    if not t:
        raise ValueError("summary is empty")
    return client.embeddings.create(model=model, input=[t]).data[0].embedding

def knn_search(conn, qvec: list, table: str, limit: int = 50):
    sql = f"""
      SELECT snippet_id, section_id, canonical_path, chunk_ix,
             page_start, page_end, citation, full_text,
             (embedding <#> %s::vector) AS distance
      FROM {table}
      ORDER BY embedding <#> %s::vector
      LIMIT %s
    """
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (qvec, qvec, limit))
        return cur.fetchall()

def stitch_by_section(rows, top_k=6):
    by = {}
    for r in rows:
        g = by.setdefault(r["section_id"], {
            "canonical_path": r["canonical_path"],
            "snippets": [], "cites": set(), "best": r["distance"]
        })
        g["snippets"].append(r)
        if r.get("citation"):
            g["cites"].add(r["citation"])
        g["best"] = min(g["best"], r["distance"])
    sections = []
    for sid, g in by.items():
        g["snippets"].sort(key=lambda x: (x["chunk_ix"], x["page_start"], x["page_end"]))
        text = "\n\n".join(x["full_text"] for x in g["snippets"] if x.get("full_text"))
        sections.append({
            "section_id": sid,
            "canonical_path": g["canonical_path"],
            "text": text.strip(),
            "citations": sorted(g["cites"]),
            "best_dist": g["best"],
        })
    sections.sort(key=lambda s: s["best_dist"])
    return sections[:top_k]

# ----- Main -----
def main():
    PG_DSN  = os.getenv("PG_DSN")
    API_KEY = os.getenv("OPENAI_API_KEY")
    if not (PG_DSN and API_KEY):
        print("PG_DSN / OPENAI_API_KEY í•„ìš”"); sys.exit(1)

    # analysis_id ì¸ì(optional): ì—†ìœ¼ë©´ created_at ìµœì‹  ìë™ ì„ íƒ
    analysis_id = sys.argv[1].strip() if len(sys.argv) >= 2 else None
    table = (len(sys.argv) > 2 and sys.argv[2]) or os.getenv("RAG_TABLE") or "ref_handbook_snippet"

    # ìµœì‹  ìë™ ì„ íƒ ì‹œ ë²”ìœ„(ì„ íƒ): TEST_USER_ID / TEST_CONV_ID
    default_user_id = os.getenv("TEST_USER_ID") or None
    default_conv_id = os.getenv("TEST_CONV_ID") or None

    client = OpenAI(api_key=API_KEY)

    with psycopg2.connect(PG_DSN) as conn:
        if not analysis_id:
            analysis_id = fetch_latest_analysis_id_by_created(conn, default_user_id, default_conv_id)
            if not analysis_id:
                print("ìµœì‹  analysis_id ì—†ìŒ"); sys.exit(1)

        row = fetch_analysis_by_id(conn, analysis_id)
        if not row:
            print("analysis_result ë ˆì½”ë“œ ì—†ìŒ"); sys.exit(1)

        summary = (row.get("summary") or "").strip()
        if not summary:
            print("summary ë¹„ì–´ìˆìŒ"); sys.exit(1)

        qvec = make_query_embedding(client, summary)
        rows = knn_search(conn, qvec, table=table, limit=50)

    # â¬‡ï¸ ì„¹ì…˜ë³„ ìµœì†Œ distance ë§µ (KNN rowsì—ì„œ ê³„ì‚°)
    best_dist_map = {}
    for r in rows:
        sid = r["section_id"]
        d = r.get("distance")
        if d is None:
            continue
        if sid not in best_dist_map or d < best_dist_map[sid]:
            best_dist_map[sid] = d

    # 1) íˆíŠ¸ëœ section_id ìˆ˜ì§‘
    hit_section_ids = sorted({r["section_id"] for r in rows})

    # 2) ì„¹ì…˜ ì „ì²´ ë³¸ë¬¸ì„ DBì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì™€ ì¬ì¡°ë¦½ (=> citeë„ ì„¹ì…˜ ì „ì²´ ê¸°ì¤€)
    with psycopg2.connect(PG_DSN) as conn2:
        sections = fetch_full_sections(conn2, table, hit_section_ids)

    # 3) ê° ì„¹ì…˜ì— KNNì—ì„œ ê³„ì‚°í•œ ìµœì†Œ distanceë¥¼ ì£¼ì…
    for s in sections:
        s["best_dist"] = best_dist_map.get(s["section_id"])

    # 4) distance ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ kê°œë§Œ ì„ íƒ
    sections.sort(key=lambda z: (float("inf") if z.get("best_dist") is None else z["best_dist"]))
    sections = sections[:6]

    print(f"\n=== ANALYSIS_ID ===\n{analysis_id}")
    print("\n=== SUMMARY (query) ===\n", summary)
    print("\n=== TOP SECTIONS ===")
    if not sections:
        print("(no sections)")

    # ğŸ”¹ PREVIEW_LINES = ëª‡ ì¤„ê¹Œì§€ ì¶œë ¥í• ì§€ 
    preview_lines = int(os.getenv("PREVIEW_LINES") or 2)

    for i, s in enumerate(sections, 1):
        lines = s["text"].splitlines()
        body = "\n".join(lines[:preview_lines])
        if len(lines) > preview_lines:
            body += "\n   â€¦ (ì´í•˜ ìƒëµ)"

        cite = "; ".join(s["citations"]) or "(no explicit page)"
        d = s.get("best_dist")
        d_str = f"{d:.4f}" if isinstance(d, (int, float)) else "-"

        print(f"{i}. {s['canonical_path']}  dist={d_str}\n   {body}\n   cite: {cite}\n")

if __name__ == "__main__":
    main()
