# -*- coding: utf-8 -*-
"""
chunking.py
- ê°€ì¥ ì‘ì€ ëª©ì°¨(leaf) ë‹¨ìœ„ë¡œ PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ 600~800ìë¡œ ì²­í‚¹
- ì…ë ¥:  toc_out/book_toc_*.jsonl, pdfë³€í™˜_downloads/*.pdf
- ì¶œë ¥:  chunking_out/snippets_{book}.jsonl, chunking_out/chunking_all.jsonl
"""
import json
import uuid
import re
import unicodedata
from pathlib import Path
from typing import List, Dict, Tuple
import fitz  # pip install pymupdf

# -------------------- ê²½ë¡œ/íŒŒë¼ë¯¸í„° --------------------
ROOT     = Path(__file__).resolve().parents[0]
PDF_DIR  = ROOT / "pdfë³€í™˜_downloads"
TOC_DIR  = ROOT / "toc_out"
OUT_DIR  = ROOT / "chunking_out"
OUT_DIR.mkdir(parents=True, exist_ok=True)

ALL_OUT_JSON = OUT_DIR / "chunking_all.jsonl"

MIN_CHARS, MAX_CHARS = 600, 800   # ìš”êµ¬ì‚¬í•­: 600~800ì
SAVE_FULL_TEXT = True             # ë³¸ë¬¸ ì €ì¥

# -------------------- ìœ í‹¸ --------------------
def safe_name(s: str) -> str:
    return re.sub(r'[\\/:*?"<>|]+', "_", s or "").strip()

def norm_text(s: str, max_len: int | None = None) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s).strip()
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€˜", "'").replace("â€™", "'")
    if max_len:
        s = s[:max_len]
    return s

def load_toc_lines(path: Path) -> List[Dict]:
    rows = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                rows.append(json.loads(line))
    return rows

def build_parent_index(rows: List[Dict]) -> Tuple[Dict[str, Dict], Dict[str, List[Dict]]]:
    """toc_id -> row, parent_toc_id -> [children] ì¸ë±ìŠ¤ ìƒì„±"""
    by_id = {r["toc_id"]: r for r in rows}
    children: Dict[str, List[Dict]] = {}
    for r in rows:
        pid = r.get("parent_toc_id")
        if pid:
            children.setdefault(pid, []).append(r)
    return by_id, children

def is_leaf(row: Dict, children_index: Dict[str, List[Dict]]) -> bool:
    """ê°€ì¥ ì‘ì€ ëª©ì°¨(leaf) íŒì •: blacklisted ì•„ë‹ˆê³  children ì—†ìŒ, level>=2"""
    if row.get("is_blacklisted", False):
        return False
    if int(row.get("level", 0)) < 2:
        return False
    return row["toc_id"] not in children_index

def collect_path_titles(row: Dict, by_id: Dict[str, Dict]) -> List[str]:
    """í˜„ì¬ row ê¸°ì¤€ìœ¼ë¡œ level 1ê¹Œì§€ parentë¥¼ íƒ€ê³  ì˜¬ë¼ê°€ ì œëª© ê²½ë¡œ ìƒì„±(ìœ„->ì•„ë˜ ì •ë ¬)"""
    path = []
    cur = row
    # ì•ˆì „ ë£¨í”„
    for _ in range(12):
        title = norm_text(cur.get("norm_title") or cur.get("title"))
        level = int(cur.get("level", 0))
        path.append((level, title))
        pid = cur.get("parent_toc_id")
        if not pid or pid not in by_id:
            break
        cur = by_id[pid]
    # level ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ í›„ ì œëª©ë§Œ ì¶”ì¶œ
    path = [t for _, t in sorted(path, key=lambda x: x[0])]
    return path  # [L1, L2, L3, ...í˜„ì¬]

def pad_path_to_3(path: List[str]) -> Tuple[str, str, str]:
    """ê²½ë¡œë¥¼ L1/L2/L3 3ì¹¸ìœ¼ë¡œ íŒ¨ë”© (ì—†ìœ¼ë©´ ë¹ˆë¬¸ì)"""
    l1 = path[0] if len(path) > 0 else ""
    l2 = path[1] if len(path) > 1 else ""
    l3 = path[2] if len(path) > 2 else ""
    return l1, l2, l3

def extract_text_by_pages(pdf_path: Path, s: int, e: int) -> str:
    """PDF í˜ì´ì§€ ë²”ìœ„(1-based, inclusive) í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    with fitz.open(str(pdf_path)) as doc:
        s = max(1, int(s)); e = max(s, int(e))
        last = doc.page_count
        if e > last:
            e = last
        parts = []
        for pg in range(s - 1, e):
            parts.append(doc.load_page(pg).get_text("text") or "")
    return "\n".join(parts)

# -------------------- ì²­í‚¹ (ë¬¸ë‹¨/ë¬¸ì¥ ê²½ê³„ ìš°ì„ , 600~800ì) --------------------
# ê³ ì • ê¸¸ì´ lookbehindë§Œ ì‚¬ìš© (íŒŒì´ì¬ re ì œì•½)
_SENT_SPLIT = re.compile(r'(?:(?<=[\.!\?ã€‚ï¼ï¼Ÿ][\'"\)\]])|(?<=[\.!\?ã€‚ï¼ï¼Ÿ]))\s+(?=[^\s])')

def split_paragraphs(text: str) -> List[str]:
    """
    ë¹ˆ ì¤„ ê¸°ì¤€ ë¬¸ë‹¨ ë¶„ë¦¬. [KEY POINT]ëŠ” ë¬¸ë‹¨ ê²½ê³„ë¡œ ì˜ë¦¬ê³  ì•ì—ì„œ ìƒˆ ë¬¸ë‹¨ ì‹œì‘ë˜ë„ë¡ ì²˜ë¦¬.
    """
    # [KEY POINT] ì•ì—ì„œ ëŠê¸° ì‰½ê²Œ ê°œí–‰ ì‚½ì…
    text = re.sub(r"\s*\[KEY POINT\]\s*", "\n[KEY POINT] ", text)
    t = text.replace("\r", "")
    blocks, cur = [], []
    for line in t.split("\n"):
        if line.strip():
            cur.append(line.strip())
        else:
            if cur:
                blocks.append(" ".join(cur).strip())
                cur = []
    if cur:
        blocks.append(" ".join(cur).strip())
    return [b for b in blocks if b]

def chunk_text_600_800(txt: str, min_len=MIN_CHARS, max_len=MAX_CHARS) -> List[str]:
    """
    ê·œì¹™:
      - ë¬¸ë‹¨ ìš°ì„ ìœ¼ë¡œ ëˆ„ì í•˜ë˜, 'ë¬¸ë‹¨ í•˜ë‚˜ê°€ max_lenì„ ë„˜ìœ¼ë©´' ë°˜ë“œì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
      - ë¬¸ì¥ë“¤ì„ ê·¸ë¦¬ë””ë¡œ ë¬¶ì–´ì„œ [min_len, max_len] ë²”ìœ„ ë§ì¶¤
      - ì–´ë–¤ ê²½ìš°ì—ë„ max_len ì´ˆê³¼ ì¡°ê°ì´ ë‚¨ì§€ ì•Šë„ë¡ ë§ˆì§€ë§‰ì— í•˜ë“œì»· ì„¸ì´í”„ê°€ë“œ
    """
    chunks: List[str] = []
    buf = ""

    def flush():
        nonlocal buf
        if buf.strip():
            chunks.append(buf.strip())
            buf = ""

    def pack_sentences(sentences: List[str]):
        """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ë¦¬ë””ë¡œ [min,max] ë²”ìœ„ë¡œ í¬ì¥í•˜ì—¬ chunksì— ì¶”ê°€"""
        nonlocal buf
        cur = buf.strip()
        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue
            cand = (cur + (" " if cur else "") + sent).strip()
            if len(cand) <= max_len:
                cur = cand
            else:
                # í˜„ì¬ í¬ì¥ì´ min ì´ìƒì´ë©´ ë‚´ë³´ë‚´ê³  ìƒˆ í¬ì¥ ì‹œì‘
                if len(cur) >= min_len:
                    chunks.append(cur)
                    cur = sent
                    # ìƒˆ ë¬¸ì¥ í•˜ë‚˜ê°€ ë„ˆë¬´ ê¸¸ë©´(ê°œë³„ ë¬¸ì¥ ìì²´ê°€ max ì´ˆê³¼) í•˜ë“œì»·
                    while len(cur) > max_len:
                        chunks.append(cur[:max_len].strip())
                        cur = cur[max_len:].strip()
                else:
                    # min ë¯¸ë§Œì¸ë° ë¶™ì´ë©´ max ì´ˆê³¼ â†’ í˜„ì¬ curë¥¼ ë°©ì¶œí•˜ê³  sentë¡œ ì‹œì‘
                    if cur:
                        chunks.append(cur)
                    cur = sent
                    while len(cur) > max_len:
                        chunks.append(cur[:max_len].strip())
                        cur = cur[max_len:].strip()
        # ë‚¨ì€ í¬ì¥ì€ bufë¡œ ëŒë ¤ ë‹¤ìŒ ë¬¸ë‹¨ê³¼ ì´ì–´ ë¶™ì„
        buf = cur

    for para in split_paragraphs(txt):
        para = para.strip()
        if not para:
            continue

        # 1) ë¬¸ë‹¨ì´ ì§§ì•„ì„œ í†µì§¸ë¡œ ë¶™ì—¬ë„ ë˜ë©´ ë²„í¼ì— ê·¸ë¦¬ë””ë¡œ ëˆ„ì 
        cand = (buf + ("\n\n" if buf else "") + para).strip()
        if len(para) <= max_len and len(cand) <= max_len:
            buf = cand
            continue

        # 2) ì—¬ê¸°ë¡œ ì™”ë‹¤ëŠ” ê±´ (a) ë¬¸ë‹¨ ë‹¨ë…ìœ¼ë¡œë„ ê¸¸ê±°ë‚˜, (b) ë²„í¼ì™€ í•©ì¹˜ë©´ ì´ˆê³¼
        #    ë¬¸ë‹¨ì´ maxë¥¼ ë„˜ê¸°ë©´ 'í•­ìƒ' ë¬¸ì¥ ë¶„í•´í•´ì„œ í¬ì¥
        if len(para) > max_len:
            sentences = _SENT_SPLIT.split(para)
            pack_sentences(sentences)
            # ì„¸ì´í”„ê°€ë“œ
            if len(buf) > max_len:
                while len(buf) > max_len:
                    chunks.append(buf[:max_len].strip())
                    buf = buf[max_len:].strip()
            continue

        # 3) ë¬¸ë‹¨ì€ ì§§ì§€ë§Œ ë²„í¼ì™€ í•©ì¹˜ë©´ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        #    ë²„í¼ê°€ min ì´ìƒì´ë©´ ë¨¼ì € ë¹„ìš°ê³ , ê·¸ ë‹¤ìŒ ìƒˆ ë¬¸ë‹¨ì„ ì‹œì‘
        if len(buf) >= min_len:
            flush()
            # ìƒˆ ë¬¸ë‹¨ ì‹œì‘
            if len(para) <= max_len:
                buf = para
            else:
                sentences = _SENT_SPLIT.split(para)
                pack_sentences(sentences)
        else:
            # bufê°€ ë„ˆë¬´ ì§§ì„ ë•ŒëŠ” ë¬¸ì¥ ë¶„í•´ë¡œ ë‘ ì¡°ê°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë§ì¶˜ë‹¤
            # (buf + para)ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¤ì‹œ í¬ì¥
            combined = (buf + (" " if buf else "") + para).strip()
            sentences = _SENT_SPLIT.split(combined)
            buf = ""  # ì¬í¬ì¥ì„ ìœ„í•´ ì´ˆê¸°í™”
            pack_sentences(sentences)

    # ë§ˆì§€ë§‰ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
    if buf:
        if len(buf) > max_len:
            while len(buf) > max_len:
                chunks.append(buf[:max_len].strip())
                buf = buf[max_len:].strip()
        if buf:
            # ë„ˆë¬´ ì§§ìœ¼ë©´ ì§ì „ ì¡°ê°ê³¼ ë³‘í•© ì‹œë„
            if chunks and len(buf) < (min_len // 2) and len(chunks[-1]) + 2 + len(buf) <= max_len:
                chunks[-1] = (chunks[-1] + "\n\n" + buf).strip()
            else:
                chunks.append(buf.strip())

    # ìµœì¢… íŠ¸ë¦¬ë° + ì„¸ì´í”„ê°€ë“œ (ì ˆëŒ€ max ì´ˆê³¼ ê¸ˆì§€)
    out = []
    for c in chunks:
        c = c.strip()
        if not c:
            continue
        if len(c) <= max_len:
            out.append(c)
        else:
            # í˜¹ì‹œ ëª¨ë¥¼ ì´ˆê³¼ëŠ” í•˜ë“œì»·
            start = 0
            while start < len(c):
                out.append(c[start:start + max_len].strip())
                start += max_len
    return out

# -------------------- ì‚¬í›„ ë³´ì •(ì ˆëŒ€ 800ì ì´ˆê³¼ ê¸ˆì§€ + ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ ìš°ì„ ) --------------------
def _smart_split_once(s: str, max_len: int) -> List[str]:
    """
    sê°€ max_lenì„ ë„˜ìœ¼ë©´ ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ì—ì„œ ë‘ ì¡°ê° ì´ìƒìœ¼ë¡œ ìë¥¸ë‹¤.
    ìš°ì„ ìˆœìœ„: [KEY POINT] / ë²ˆí˜¸Â·ë¶ˆë¦¿ / ë¬¸ì¥ë¶€í˜¸ / ê³µë°± / ìµœí›„ìˆ˜ë‹¨ í•˜ë“œì»·
    ì¬ê·€ì ìœ¼ë¡œ í˜¸ì¶œëœë‹¤.
    """
    s = s.strip()
    if len(s) <= max_len:
        return [s] if s else []

    # 1) [KEY POINT] ì•ì—ì„œ ìë¥´ê¸° (ê°€ëŠ¥í•œ ê°€ì¥ ì˜¤ë¥¸ìª½ ê²½ê³„)
    idx = s.rfind("[KEY POINT]", 0, max_len)
    if idx > 0:
        left = s[:idx].rstrip()
        right = s[idx:].lstrip()
        return ([left] if left else []) + _smart_split_once(right, max_len)

    # 2) ë²ˆí˜¸/ë¶ˆë¦¿ ê²½ê³„
    for pat in [r"\n+\s*\d+\s", r"\s\d+\s", r"\sâ€¢\s", r"\s-\s"]:
        m = list(re.finditer(pat, s))
        cut = max([mm.start() for mm in m if mm.start() < max_len], default=-1)
        if cut > 0:
            left = s[:cut].rstrip()
            right = s[cut:].lstrip()
            return ([left] if left else []) + _smart_split_once(right, max_len)

    # 3) ë¬¸ì¥ë¶€í˜¸(ê°€ì¥ ì˜¤ë¥¸ìª½)ì—ì„œ ìë¥´ê¸°
    punct = max(s.rfind(p, 0, max_len) for p in [".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ"])
    if punct > 0:
        left = s[:punct+1].rstrip()
        right = s[punct+1:].lstrip()
        return ([left] if left else []) + _smart_split_once(right, max_len)

    # 4) ê³µë°±ì—ì„œ ìë¥´ê¸°
    space = s.rfind(" ", 0, max_len)
    if space > 0:
        left = s[:space].rstrip()
        right = s[space+1:].lstrip()
        return ([left] if left else []) + _smart_split_once(right, max_len)

    # 5) ìµœí›„ìˆ˜ë‹¨: í•˜ë“œì»·
    left = s[:max_len].rstrip()
    right = s[max_len:].lstrip()
    return ([left] if left else []) + _smart_split_once(right, max_len)

def enforce_length(chunks: List[str], min_len: int, max_len: int) -> List[str]:
    """
    - ëª¨ë“  ì¡°ê°ì´ max_len ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ê°•ì œ ë¶„í• 
    - ë§ˆì§€ë§‰ ê¼¬ë¦¬ ì¡°ê°ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì§ì „ê³¼ ë³‘í•©(ê°€ëŠ¥í•˜ë©´)
    """
    fixed: List[str] = []
    for c in chunks:
        parts = _smart_split_once(c.strip(), max_len)
        fixed.extend(p for p in parts if p.strip())

    # ë§ˆì§€ë§‰ ê¼¬ë¦¬ ë³‘í•© ì‹œë„
    if len(fixed) >= 2 and len(fixed[-1]) < (min_len // 2):
        tail = fixed.pop()
        if len(fixed[-1]) + 2 + len(tail) <= max_len:
            fixed[-1] = (fixed[-1] + "\n\n" + tail).strip()
        else:
            fixed.append(tail)
    return fixed

# -------------------- ë©”ì¸ ë¡œì§ --------------------
def process_one_book(toc_jsonl_path: Path, all_fw) -> int:
    rows = load_toc_lines(toc_jsonl_path)
    if not rows:
        return 0

    by_id, children_index = build_parent_index(rows)
    pdf_names = sorted({r["source_file"] for r in rows})
    book_title = rows[0].get("book_title") or Path(pdf_names[0]).stem
    book_title = norm_text(book_title)
    out_path = OUT_DIR / f"snippets_{safe_name(book_title)}.jsonl"

    total_written = 0
    with out_path.open("w", encoding="utf-8") as fw:
        for pdf_name in pdf_names:
            pdf_path = PDF_DIR / pdf_name
            if not pdf_path.exists():
                continue

            # leaf ì„¹ì…˜ë§Œ íƒ€ê¹ƒ
            targets = [r for r in rows if r.get("source_file") == pdf_name and is_leaf(r, children_index)]
            # ë¬¸ì„œìƒ ìˆœì„œ ë³´ì¥
            targets.sort(key=lambda x: int(x.get("order_ix", 0)))

            for leaf in targets:
                s, e = int(leaf["page_start"]), int(leaf["page_end"])
                raw_text = extract_text_by_pages(pdf_path, s, e)
                raw_text = norm_text(raw_text)
                if not raw_text:
                    continue

                # ê²½ë¡œ/ì œëª©
                path_titles = collect_path_titles(leaf, by_id)  # [L1, L2, L3, ..., leaf]
                l1, l2, l3 = pad_path_to_3(path_titles[:3])
                display_path = " > ".join([t for t in [l1, l2, l3] if t])

                # ë³¸ë¬¸ ì²­í‚¹ (600~800ì)
                chunks = chunk_text_600_800(raw_text, MIN_CHARS, MAX_CHARS)
                # ì‚¬í›„ ë³´ì •ìœ¼ë¡œ ì ˆëŒ€ 800 ì´ˆê³¼ ê¸ˆì§€ + ê¼¬ë¦¬ ë³‘í•©
                chunks = enforce_length(chunks, MIN_CHARS, MAX_CHARS)
                if not chunks:
                    continue

                # ë©”íƒ€
                book_id   = leaf.get("book_id")
                toc_id    = leaf.get("toc_id")
                citation  = norm_text(f"{(leaf.get('norm_title') or leaf.get('title') or '').strip()} p.{s}-{e}", 400)

                for ix, ck in enumerate(chunks):
                    rec = {
                        "snippet_id": str(uuid.uuid4()),
                        "book_id": book_id,
                        "book_title": book_title,
                        "l1_title": l1,
                        "l2_title": l2,
                        "l3_title": l3,
                        "canonical_path": display_path,
                        "section_id": toc_id,       # leaf ë‹¨ìœ„
                        "chunk_ix": ix,             # 0-based
                        "page_start": s,
                        "page_end": e,
                        "citation": citation,
                        "full_text": ck if SAVE_FULL_TEXT else None,
                        # ë‚˜ì¤‘ ì„ë² ë”© ì‹œ ê·¸ëŒ€ë¡œ ì“°ë©´ ë¨
                        "embed_text": f"[{display_path}] {ck}",
                        "embedding": None
                    }
                    line = json.dumps(rec, ensure_ascii=False)
                    fw.write(line + "\n")
                    all_fw.write(line + "\n")
                    total_written += 1

    print(f"âœ… {book_title} ì²­í‚¹ ì €ì¥: {out_path} ({total_written} rows)")
    return total_written

def run():
    toc_files = sorted(TOC_DIR.glob("book_toc_*.jsonl"))
    if not toc_files:
        raise FileNotFoundError(f"TOC íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {TOC_DIR}")

    grand_total = 0
    with ALL_OUT_JSON.open("w", encoding="utf-8") as all_fw:
        for toc_jsonl in toc_files:
            grand_total += process_one_book(toc_jsonl, all_fw)

    print(f"\nğŸ“¦ í•©ë³¸ ì €ì¥: {ALL_OUT_JSON} (ì´ {grand_total} rows)")

if __name__ == "__main__":
    run()
