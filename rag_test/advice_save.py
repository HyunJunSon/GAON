# -*- coding: utf-8 -*-
# advice_save.py

import textwrap
import os, sys, json
import psycopg2
import psycopg2.extras as extras
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# ----- .env ë¡œë”© (í˜„ì¬ í´ë”/ìƒìœ„ í´ë” ìš°ì„  íƒìƒ‰) -----
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

# ----- DB helpers -----
def fetch_analysis_by_id(conn, analysis_id: str) -> dict:
    sql = "SELECT * FROM analysis_result WHERE analysis_id = %s LIMIT 1"
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (analysis_id,))
        row = cur.fetchone()
    if not row:
        return {}
    for k in ("statistics", "style_analysis"):
        if isinstance(row.get(k), str):
            try: row[k] = json.loads(row[k])
            except: pass
    return row

def fetch_latest_analysis_id_by_created(conn, user_id: str|None=None, conv_id: str|None=None) -> str|None:
    base = "SELECT analysis_id FROM analysis_result"
    where, params = [], []
    if user_id:
        where.append("user_id = %s"); params.append(user_id)
    if conv_id:
        where.append("conv_id = %s"); params.append(conv_id)
    where_sql = (" WHERE " + " AND ".join(where)) if where else ""
    sql = f"""{base}{where_sql}
              ORDER BY created_at DESC NULLS LAST
              LIMIT 1"""
    with conn.cursor() as cur:
        cur.execute(sql, params)
        row = cur.fetchone()
    return row[0] if row else None

def fetch_full_sections(conn, table: str, section_ids: list[str]) -> list[dict]:
    """
    ì£¼ì–´ì§„ section_id ë“¤ì— ëŒ€í•´, ì„¹ì…˜ì— ì†í•œ ëª¨ë“  ìŠ¤ë‹ˆí«ì„ ê°€ì ¸ì™€
    chunk_ix / page ìˆœìœ¼ë¡œ ì •ë ¬ â†’ ì „ì²´ ë³¸ë¬¸/ì¸ìš©ì„ ì¬ì¡°ë¦½.
    """
    if not section_ids:
        return []

    sql = f"""
      SELECT section_id, canonical_path, chunk_ix,
             page_start, page_end, citation, full_text
      FROM {table}
      WHERE section_id = ANY(%s)
      ORDER BY section_id, chunk_ix, page_start, page_end
    """
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (section_ids,))
        rows = cur.fetchall()

    by = {}
    for r in rows:
        g = by.setdefault(r["section_id"], {
            "section_id": r["section_id"],
            "canonical_path": r["canonical_path"],
            "snippets": [],
            "citations": set(),
        })
        g["snippets"].append(r)
        if r.get("citation"):
            g["citations"].add(r["citation"])

    sections = []
    for sec in by.values():
        text = "\n\n".join(x["full_text"] for x in sec["snippets"] if x.get("full_text"))
        sections.append({
            "section_id": sec["section_id"],
            "canonical_path": sec["canonical_path"],
            "text": text.strip(),
            "citations": sorted(sec["citations"]),
        })
    return sections

# ----- Embedding / KNN / Stitch -----
def make_query_embedding(client: OpenAI, text: str, model="text-embedding-3-small") -> list:
    t = (text or "").strip()
    if not t:
        raise ValueError("summary is empty")
    return client.embeddings.create(model=model, input=[t]).data[0].embedding

def knn_search(conn, qvec: list, table: str, limit: int = 50):
    sql = f"""
      SELECT snippet_id, section_id, canonical_path, chunk_ix,
             page_start, page_end, citation, full_text,
             (embedding <#> %s::vector) AS distance
      FROM {table}
      ORDER BY embedding <#> %s::vector
      LIMIT %s
    """
    with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:
        cur.execute(sql, (qvec, qvec, limit))
        return cur.fetchall()

def stitch_by_section(rows, top_k=6):
    by = {}
    for r in rows:
        g = by.setdefault(r["section_id"], {
            "canonical_path": r["canonical_path"],
            "snippets": [], "cites": set(), "best": r["distance"]
        })
        g["snippets"].append(r)
        if r.get("citation"):
            g["cites"].add(r["citation"])
        g["best"] = min(g["best"], r["distance"])
    sections = []
    for sid, g in by.items():
        g["snippets"].sort(key=lambda x: (x["chunk_ix"], x["page_start"], x["page_end"]))
        text = "\n\n".join(x["full_text"] for x in g["snippets"] if x.get("full_text"))
        sections.append({
            "section_id": sid,
            "canonical_path": g["canonical_path"],
            "text": text.strip(),
            "citations": sorted(g["cites"]),
            "best_dist": g["best"],
        })
    sections.sort(key=lambda s: s["best_dist"])
    return sections[:top_k]

# ----- DB Update
def update_feedback(conn, analysis_id: str, feedback_text: str):
    sql = "UPDATE analysis_result SET feedback = %s, updated_at = NOW() WHERE analysis_id = %s"
    with conn.cursor() as cur:
        cur.execute(sql, (feedback_text, analysis_id))
    conn.commit()

# ----- Main -----
def main():
    PG_DSN  = os.getenv("PG_DSN")
    API_KEY = os.getenv("OPENAI_API_KEY")
    if not (PG_DSN and API_KEY):
        print("PG_DSN / OPENAI_API_KEY í•„ìš”"); sys.exit(1)

    # analysis_id ì¸ì(optional): ì—†ìœ¼ë©´ created_at ìµœì‹  ìë™ ì„ íƒ
    analysis_id = sys.argv[1].strip() if len(sys.argv) >= 2 else None
    table = (len(sys.argv) > 2 and sys.argv[2]) or os.getenv("RAG_TABLE") or "ref_handbook_snippet"

    # ìµœì‹  ìë™ ì„ íƒ ì‹œ ë²”ìœ„(ì„ íƒ): TEST_USER_ID / TEST_CONV_ID
    default_user_id = os.getenv("TEST_USER_ID") or None
    default_conv_id = os.getenv("TEST_CONV_ID") or None

    client = OpenAI(api_key=API_KEY)

    with psycopg2.connect(PG_DSN) as conn:
        if not analysis_id:
            analysis_id = fetch_latest_analysis_id_by_created(conn, default_user_id, default_conv_id)
            if not analysis_id:
                print("ìµœì‹  analysis_id ì—†ìŒ"); sys.exit(1)

        row = fetch_analysis_by_id(conn, analysis_id)
        if not row:
            print("analysis_result ë ˆì½”ë“œ ì—†ìŒ"); sys.exit(1)

        summary = (row.get("summary") or "").strip()
        if not summary:
            print("summary ë¹„ì–´ìˆìŒ"); sys.exit(1)

        qvec = make_query_embedding(client, summary)
        rows = knn_search(conn, qvec, table=table, limit=50)

    # â¬‡ï¸ ì„¹ì…˜ë³„ ìµœì†Œ distance ë§µ (KNN rowsì—ì„œ ê³„ì‚°)
    best_dist_map = {}
    for r in rows:
        sid = r["section_id"]
        d = r.get("distance")
        if d is None:
            continue
        if sid not in best_dist_map or d < best_dist_map[sid]:
            best_dist_map[sid] = d

    # 1) íˆíŠ¸ëœ section_id ìˆ˜ì§‘
    hit_section_ids = sorted({r["section_id"] for r in rows})

    # 2) ì„¹ì…˜ ì „ì²´ ë³¸ë¬¸ì„ DBì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì™€ ì¬ì¡°ë¦½ (=> citeë„ ì„¹ì…˜ ì „ì²´ ê¸°ì¤€)
    with psycopg2.connect(PG_DSN) as conn2:
        sections = fetch_full_sections(conn2, table, hit_section_ids)

    # 3) ê° ì„¹ì…˜ì— KNNì—ì„œ ê³„ì‚°í•œ ìµœì†Œ distanceë¥¼ ì£¼ì…
    for s in sections:
        s["best_dist"] = best_dist_map.get(s["section_id"])

    # 4) distance ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ kê°œë§Œ ì„ íƒ
    sections.sort(key=lambda z: (float("inf") if z.get("best_dist") is None else z["best_dist"]))
    sections = sections[:6]

    # ---------------- LLM í”„ë¡¬í”„íŠ¸ ì‘ì„± + í˜¸ì¶œ ----------------
    # ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ ë§Œë“¤ê¸°
    ctx_blocks = []
    for i, s in enumerate(sections, 1):
        txt = s["text"]  # ì„¹ì…˜ ì „ì²´ ë³¸ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        cite = "; ".join(s["citations"]) or "(no explicit page)"
        ctx_blocks.append(f"[Context {i}] {s['canonical_path']}\n{txt}\n(ì¶œì²˜: {cite})")
    ctx_str = "\n\n".join(ctx_blocks)

    system = "You are a kind, concise, and practical Korean communication coach. Ground advice in the provided contexts."
    user = f"""
    ìµœê·¼ ëŒ€í™” ë¶„ì„ ìš”ì•½:
    \"\"\"{summary}\"\"\"

    ì•„ë˜ ì°¸ê³  ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ê¸ˆ ì‚¬ìš©ìê°€ ë‹¹ì¥ ì“¸ ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ì‘ì„±í•´ì¤˜.

    {ctx_str}

    ì§€ì¹¨:
    1) ë¨¼ì € 2~3ë¬¸ì¥ í•µì‹¬ ì¡°ì–¸.
    2) ì´ì–´ì„œ 3~6ê°œì˜ ì‹¤í–‰ ìŠ¤í…(í‘œí˜„ ì˜ˆì‹œ í¬í•¨).
    3) ì£¼ì˜í•  ì  2~3ê°€ì§€.
    4) 3ì¤„ ì²´í¬ë¦¬ìŠ¤íŠ¸.
    5) ëì— ì°¸ê³  ë¬¸ë§¥ ì¶œì²˜ë¥¼ ë¶ˆë¦¿ ëª©ë¡ìœ¼ë¡œ.
    
    ğŸ“Œ ì¶”ê°€ ê·œì¹™:
    - ê° ì‹¤í–‰ ìŠ¤í…ì—ëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ ì§ì ‘ ì¸ìš©(ì§§ì€ ë¬¸ì¥ 10~20ì)ì„ í¬í•¨í•˜ê³ , ë”°ì˜´í‘œ(" ")ë¡œ í‘œì‹œí•  ê²ƒ.
    - ê° ì¡°ì–¸ ìŠ¤í… ëì— (from: Context n) í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë§¤í•‘ì„ ëª…ì‹œí•  ê²ƒ.
    - í–‰ë™ ìŠ¤í…ì—ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë¹ˆë„(ì˜ˆ: í•˜ë£¨ 1íšŒ, 5ë¶„ê°„, 3íšŒ ë°˜ë³µ ë“±)ë¥¼ í¬í•¨í•  ê²ƒ.

    """.strip()

    print("\n=== ANSWER ===\n")
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.35,
        messages=[
            {"role":"system","content":system},
            {"role":"user","content":user},
        ],
    )
    advice_text = (resp.choices[0].message.content or "").strip()
    print(advice_text)

    # --- ì—¬ê¸°ì„œ feedback ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ ---
    try:
        with psycopg2.connect(PG_DSN) as conn3:
            update_feedback(conn3, analysis_id, advice_text)
        print("\n[OK] feedback ì €ì¥ ì™„ë£Œ â†’ analysis_result.feedback ì—…ë°ì´íŠ¸ë¨")
    except Exception as e:
        print(f"\n[ê²½ê³ ] feedback ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    # ì¶œì²˜ë§Œ ë”°ë¡œ ì •ë¦¬ (ì¤‘ë³µ ì œê±°)
    print("\n--- Sources ---")
    seen = set()
    for s in sections:
        for c in s["citations"]:
            key = (s["canonical_path"], c)
            if key in seen: 
                continue
            seen.add(key)
            print(f"- {key[0]} â€” {key[1] or '(no explicit page)'}")

    print(f"\n=== ANALYSIS_ID ===\n{analysis_id}")
    print("\n=== SUMMARY (query) ===\n", summary)
    print("\n=== TOP SECTIONS ===")
    if not sections:
        print("(no sections)")
        
    # ğŸ”¹ PREVIEW_LINES = ëª‡ ì¤„ê¹Œì§€ ì¶œë ¥í• ì§€ 
    preview_lines = int(os.getenv("PREVIEW_LINES") or 2)

    for i, s in enumerate(sections, 1):
        lines = s["text"].splitlines()
        body = "\n".join(lines[:preview_lines])
        if len(lines) > preview_lines:
            body += "\n   â€¦ (ì´í•˜ ìƒëµ)"

        cite = "; ".join(s["citations"]) or "(no explicit page)"
        d = s.get("best_dist")
        d_str = f"{d:.4f}" if isinstance(d, (int, float)) else "-"

        print(f"{i}. {s['canonical_path']}  dist={d_str}\n   {body}\n   cite: {cite}\n")

if __name__ == "__main__":
    main()
