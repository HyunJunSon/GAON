# llm_evaluate
### evaluate process
Embedding-> Retriever -> Generator -> Judge/Evaluator -> Formatter -> Simulator

<details>
<summary>setup</summary>
<pre>pip install -U deepeval openai langchain chromadb
export OPENAI_API_KEY=sk-*** </pre>

data: oh_eunyoung.txt, family_dialogues.json

</details>

<details>
<summary>Embedding</summary>
ëª©ì : text-embedding-3-large ê°€ â€œì˜ë¯¸ ìœ ì‚¬ë„ ê²€ìƒ‰â€ì— ì í•©í•œì§€ í™•ì¸ (= ê²°ê³¼ì ìœ¼ë¡œ Retriever í’ˆì§ˆë¡œ ë“œëŸ¬ë‚¨)

í…ŒìŠ¤íŠ¸ ë°©ì‹

ì¿¼ë¦¬(=ëŒ€í™” ìš”ì•½ or í•µì‹¬ ì§ˆë¬¸)ì™€ ì •ë‹µ ê·¼ê±° ë¬¸ë‹¨(ê³¨ë“ )ì„ ì¤€ë¹„

í•´ë‹¹ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“  ë²¡í„°DBì—ì„œ top-k ê²€ìƒ‰ ê²°ê³¼ë¥¼ test_case.retrieval_contextì— ë„£ê³  í‰ê°€

íŒŒì¼: tests/test_embedding_retriever.py

ì‹¤í–‰:
<pre>deepeval test run tests/test_embedding_retriever.py -v</pre>

ğŸ‘‰ ì„ë² ë”© ëª¨ë¸ êµì²´ í…ŒìŠ¤íŠ¸: ë™ì¼ ì½”ë“œì—ì„œ ìƒì„±í•œ ë²¡í„°DBë§Œ ë°”ê¿” ëŒë ¤ ì ìˆ˜ ë¹„êµ(OpenAI vs Gemini Embedding ë“±).
</details>

<details>
<summary>Retriever</summary>
ëª©ì : ê°™ì€ ì„ë² ë”©ì´ë¼ë„ ê²€ìƒ‰ ì „ëµ/íŒŒë¼ë¯¸í„°(k, rerank) ê°€ ë§ëŠ”ì§€ ê²€ì¦

íŠ¸ë ˆì´ì‹±ìœ¼ë¡œ ì»´í¬ë„ŒíŠ¸ í‰ê°€

@observe(metrics=[ContextualRelevancyMetric()]) ë¥¼ retriever í•¨ìˆ˜ì— ë¶€ì°©

í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ update_current_span(test_case=LLMTestCase(...)) ë¡œ retrieval_context ê¸°ë¡

íŒŒì¼: tests/test_retriever_component.py

ì‹¤í–‰:
<pre>python tests/test_retriever_component.py</pre>

</details>

<details>
<summary>Generator</summary>
ëª©ì : GPT-4o / Claude / Gemini ì¤‘ ëˆ„ê°€ â€œëŒ€í™” ë¶„ì„Â·ì¡°ì–¸â€ì„ ë” ì˜ ìƒì„±í•˜ëŠ”ì§€

ë©”íŠ¸ë¦­

Faithfulness: ë‹µì´ ê²€ìƒ‰ ë¬¸ë§¥ì— ê¸°ë°˜í–ˆë‚˜

Answer Relevancy: ì§ˆë¬¸/ëŒ€í™”ì™€ ê´€ë ¨ ìˆë‚˜

GEval(ì»¤ìŠ¤í…€): ê³µê°(Empathy)Â·êµ¬ì²´ì„±(Actionability) ì±„ì 

íŒŒì¼: tests/test_generator_models.py

ì‹¤í–‰:
<pre>deepeval test run tests/test_generator_models.py -v</pre>

ğŸ‘‰ í‰ê·  ì ìˆ˜ë¡œ Generator ìš°ìŠ¹ ê²°ì •.

</details>

<details>
<summary>Judge/Evaluator</summary>
ëª©ì : ì‹¬íŒ LLM í›„ë³´(GPT-4o mini vs Claude Haiku) ì˜ ì¼ê´€ì„±/ì•ˆì •ì„± ë¹„êµ

ë°©ë²•

ë™ì¼í•œ casesì— ëŒ€í•´ ë‘ ì‹¬íŒìœ¼ë¡œ ê°ê° í‰ê°€ â†’ ì ìˆ˜ ìƒê´€(Spearman)Â·ì¬í˜„ì„±(ì¬ì‹¤í–‰ ë¶„ì‚°)Â·ì—ëŸ¬ìœ¨(429/JSON ì‹¤íŒ¨) ë¡œê·¸ ë¹„êµ

íŒŒì¼: tests/test_judges_agreement.py

ğŸ‘‰ ë‘ ì±„ì í‘œì˜ ìˆœìœ„/ìƒê´€ì„ ë³´ê³  ì‹¬íŒ ì±„íƒ.

</details>

<details>
<summary>Formatter</summary>
ëª©ì : ë³´ê³ ì„œ ê°€ë…ì„±/êµ¬ì¡° ì¤€ìˆ˜/JSON í¬ë§· ì •í™•ì„±

ë©”íŠ¸ë¦­ ì•„ì´ë””ì–´

GEval(Clarity/Structure): â€œìš”ì•½â†’ì§„ë‹¨â†’ì œì•ˆâ†’ì ìˆ˜â€ êµ¬ì¡° ì¤€ìˆ˜

JSON Correctness (ì¶œë ¥ì„ JSONìœ¼ë¡œ ê°•ì œí•˜ëŠ” ê²½ìš°)

Hallucination/Consistency: ë³´ê³ ì„œì˜ ê·¼ê±° ì¼ì¹˜

íŒŒì¼: tests/test_formatter_quality.py


</details>

<details>
<summary>Simulator</summary>
ëª©ì : ë³´ê³ ì„œ ê°€ë…ì„±/êµ¬ì¡° ì¤€ìˆ˜/JSON í¬ë§· ì •í™•ì„±

ë©”íŠ¸ë¦­ ì•„ì´ë””ì–´

GEval(Clarity/Structure): â€œìš”ì•½â†’ì§„ë‹¨â†’ì œì•ˆâ†’ì ìˆ˜â€ êµ¬ì¡° ì¤€ìˆ˜

JSON Correctness (ì¶œë ¥ì„ JSONìœ¼ë¡œ ê°•ì œí•˜ëŠ” ê²½ìš°)

Hallucination/Consistency: ë³´ê³ ì„œì˜ ê·¼ê±° ì¼ì¹˜

íŒŒì¼: tests/test_formatter_quality.py

ğŸ‘‰ ë™ì¼ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëª¨ë¸A/B/C ì¶œë ¥ìœ¼ë¡œ ê°ê° ëŒë ¤ ì ìˆ˜ ë¹„êµ + ë³„ë„ ë¡œê¹…ìœ¼ë¡œ í‰ê·  ì‘ë‹µì‹œê°„ë„ í•¨ê»˜ ë¹„êµ.

</details>
